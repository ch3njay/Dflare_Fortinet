# training_pipeline/combo_optimizer.py
from __future__ import annotations

import os
import json
import itertools
from typing import Any, Dict, List, Sequence, Tuple, Optional
import warnings
import numpy as np

from sklearn.ensemble import StackingClassifier, VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import (
    accuracy_score,
    f1_score,
    precision_score,
    recall_score,
    roc_auc_score,
    classification_report,
    confusion_matrix,
)
from sklearn.base import clone
from joblib import parallel_backend, dump

from .ensemble_optuna import OptunaEnsembler

try:
    from sklearn.exceptions import UndefinedMetricWarning
except Exception:
    UndefinedMetricWarning = Warning  # ÂÖºÂÆπËàäÁâà


# ---------------- Â∑•ÂÖ∑ÔºöCPU ÁâàÔºàÈÅøÂÖç Stacking/Voting ÈáçÊñ∞Ë®ìÁ∑¥ clone ÊôÇËß∏ÁôºÂñÆÂç° GPU Ë°ùÁ™ÅÔºâ ----------------
def _set_if_has(est, **pairs):
    """Ëã• est ÊîØÊè¥Ë©≤ÂèÉÊï∏ÔºåÊâçË®≠ÂÆöÔºõ‰∏ç‰∏ü‰æãÂ§ñ„ÄÇ"""
    try:
        est.set_params(**{k: v for k, v in pairs.items() if k in est.get_params()})
    except Exception:
        pass
    return est

def _as_cpu_estimator(est):
    """
    Â∞áÂ∏∏Ë¶ãÊ®πÁ≥ªÊ®°ÂûãÁöÑÂπ≥Ë°åÂ∫¶ËàáË£ùÁΩÆÊî∂ÊñÇÂà∞ CPU + ÂñÆÂü∑Ë°åÁ∑íÔºåÈÅøÂÖçÂñÆÂç° GPU/Â§öÈÄ≤Á®ãË°ùÁ™Å„ÄÇ
    ÂÉÖÂú®ÂèÉÊï∏Â≠òÂú®ÊôÇÊâçË®≠ÂÆöÔºå‰∏çÊîπËÆäÊú™Áü•ÂèÉÊï∏„ÄÇ
    """
    e = clone(est)
    name = e.__class__.__name__.lower()

    # Sklearn Â∏∏Ë¶ãÊ®°ÂûãÔºöËã•Êúâ n_jobs ‚Üí Ë®≠ 1
    if hasattr(e, "get_params") and "n_jobs" in e.get_params():
        _set_if_has(e, n_jobs=1)

    # CatBoost
    if "catboost" in name:
        _set_if_has(e, task_type="CPU", devices="", verbose=False)

    # XGBoost sklearn API
    if "xgb" in name or "xgboost" in name:
        # Ëã•‰Ω†Ë¶Å GPU Ë®ìÁ∑¥ÔºåÈÄôË£°ÂèØ‰æùÈúÄÊ±ÇË™øÊï¥„ÄÇÁÇ∫‰∫Ü ensemble ÂÆâÂÖ®Ôºå‰ªç‰ª• CPU Êé®Ë´ñ/ÈáçË®ì„ÄÇ
        _set_if_has(e, tree_method="hist", device="cpu", n_jobs=1, verbosity=0)

    return e


# ---------------- Ë©ïÂàÜÂ∑•ÂÖ∑ÔºàÈùúÈü≥ UndefinedMetricWarningÔºåzero_division=0Ôºâ ----------------
def _compute_metrics(task: str, y_true, y_pred, proba: Optional[np.ndarray]) -> Dict[str, Any]:
    with warnings.catch_warnings():
        warnings.filterwarnings("ignore", category=UndefinedMetricWarning)
        acc = accuracy_score(y_true, y_pred)

        if task == "binary":
            f1 = f1_score(y_true, y_pred, average="binary", zero_division=0)
            prec = precision_score(y_true, y_pred, average="binary", zero_division=0)
            rec = recall_score(y_true, y_pred, average="binary", zero_division=0)
            report = classification_report(y_true, y_pred, digits=4, zero_division=0)
        else:
            f1 = f1_score(y_true, y_pred, average="macro", zero_division=0)
            prec = precision_score(y_true, y_pred, average="macro", zero_division=0)
            rec = recall_score(y_true, y_pred, average="macro", zero_division=0)
            report = classification_report(y_true, y_pred, digits=4, zero_division=0)

    auc = np.nan
    try:
        if proba is not None:
            if task == "binary":
                pos_idx = 1 if proba.shape[1] > 1 else 0
                auc = roc_auc_score(y_true, proba[:, pos_idx])
            else:
                auc = roc_auc_score(y_true, proba, multi_class="ovr")  # Â§öÈ°ûÂà•Êé° OVR-AUC
    except Exception:
        pass

    cm = confusion_matrix(y_true, y_pred)

    # È°çÂ§ñÔºöÈ†êÊ∏¨È°ûÂà•ÂàÜ‰Ωà
    try:
        uniq, cnts = np.unique(y_pred, return_counts=True)
        pred_dist = dict(zip(uniq.tolist(), cnts.tolist()))
    except Exception:
        pred_dist = {}

    return {
        "acc": acc,
        "auc": auc,
        "f1": f1,
        "precision": prec,
        "recall": rec,
        "report": report,
        "confusion_matrix": cm,
        "pred_dist": pred_dist,
    }


class ComboOptimizer:
    """
    ÈõÜÊàêÁµÑÂêàÂô®ÔºàÊµÅÁ®ãÔºâÔºö
    1) ËÆÄÂèñ ENSEMBLE_SETTINGSÔºåÊ±∫ÂÆö Voting/Stacking Ëàá CV„ÄÇ
    2) Â∞áÂü∫Ê®°Âûã clone Êàê„ÄåCPUÔºãÂñÆÂü∑Ë°åÁ∑íÁâà„Äç‰ª•ÈÅøÂÖçÂñÆÂç° GPU Ë°ùÁ™Å„ÄÇ
    3) Âª∫Á´ãÈõÜÊàêÂô®Ôºö
       - VotingClassifier(voting='soft' Êàñ 'hard')
       - Êàñ StackingClassifier(cv=K, final_est=LogisticRegression)
    4) ÂèØÈÅ∏ÔºöVoting Â≠êÈõÜÊéíÂàóÁµÑÂêàÊêúÂ∞ãÔºåÂàóÂá∫ Top-K ‰∏¶‰øùÂ≠òÊúÄ‰Ω≥ÔºàOptuna ÊàñÂõ∫ÂÆöÊûöËàâÔºâ„ÄÇ
    5) È©óË≠âÔºöËº∏Âá∫ ACC/AUC/F1„ÄÅÂàÜÈ°ûÂ†±Âëä„ÄÅÊ∑∑Ê∑ÜÁü©Èô£ËàáÈ†êÊ∏¨ÂàÜ‰Ωà„ÄÇ
    6) È°çÂ§ñÂõûÂÇ≥/Âç∞Âá∫„Äåüß© Ensemble ÁµÑÊàê„ÄçÔºöVoting Ê¨äÈáçÊàñ Stacking ‰øÇÊï∏„ÄÇ
    7) ‰øùÂ≠òÁî¢Áâ©Ôºö
       - models/ensemble_best.joblib
       - reports/ensemble_search_report.jsonÔºàËã•ÂïüÁî®Â≠êÈõÜÊêúÂ∞ãÔºâ
       - reports/ensemble_topk.txtÔºàTop-K ÂèØËÆÄÁâàÔºâ
       - reports/ensemble_final.txtÔºàÊúÄÁµÇÁµÑÂêàÔºãÊàêÁ∏æÔºâ
    """

    def __init__(
        self,
        estimators: Sequence[Tuple[str, Any]],
        X_train,
        y_train,
        X_valid,
        y_valid,
        task_type: str = "binary",
        config: Optional[Dict[str, Any]] = None,
        logger: Any = None,
        out_dir: Optional[str] = None,
        use_optuna: bool = False,   # ‚òÖ Ëàá pipeline ‰∏≤Êé•
    ) -> None:
        self.base_estimators = list(estimators)
        self.X_train = X_train
        self.y_train = y_train
        self.X_valid = X_valid
        self.y_valid = y_valid
        self.task_type = task_type
        self.config = config or {}
        self.logger = logger
        self.out_dir = out_dir or "./artifacts"
        self.use_optuna = bool(use_optuna)

        ens = (self.config.get("ENSEMBLE_SETTINGS") or {}).copy()
        ens.setdefault("STACK_CV", 5)
        ens.setdefault("VOTING", "soft")
        ens.setdefault("THRESHOLD", 0.5)
        # Voting ÊêúÂ∞ãÂèÉÊï∏
        ens.setdefault("SEARCH", "none")   # "none" / "voting_subsets"
        ens.setdefault("SEARCH_MAX_SUBSET", 4)       # Â≠êÈõÜÊúÄÂ§ßÊ®°ÂûãÊï∏
        ens.setdefault("SEARCH_TOPK", 3)             # Âèñ Top-K
        # Optuna ÊêúÂ∞ãÂõûÂêàÔºàÂÉÖÁî®Êñº ensembleÔºâ
        ens.setdefault("OPTUNA_TRIALS", 30)
        ens.setdefault("WEIGHT_MODE", "dirichlet")
        ens.setdefault("MODE", "free")
        ens.setdefault("MIN_MODELS", 2)
        ens.setdefault("MAX_MODELS", None)
        ens.setdefault("PRUNING", True)
        ens.setdefault("DIRECTION", "maximize")
        ens.setdefault("SEED", 42)
        if not self.use_optuna:
            ens.pop("OPTUNA_TRIALS", None)
        self.ens = ens

        # ‰æõ Stacking ÁµÑÊàêËß£ËÆÄÁî®ÁöÑÈ°ûÂà•ÂàóË°®Ôºà‰øùÊåÅÊ±∫ÂÆöÂºèÔºâ
        self.classes_ = np.unique(self.y_train)

    def optimize(self) -> Dict[str, Any]:
        print(f"‚öôÔ∏è  Ensemble Ë®≠ÂÆöËºâÂÖ•ÂÆåÊàêÔºö{self.ens}")
        if not self.use_optuna:
            print("üö´ Optuna Êú™ÂïüÁî®Ôºå‰ΩøÁî®Êó¢ÂÆöÈõÜÊàêÁ≠ñÁï•„ÄÇ")

        voting_mode = str(self.ens.get("VOTING", "soft")).lower()
        stack_cv = int(self.ens.get("STACK_CV", 5))

        # Âª∫Á´ã CPU ÁâàÁöÑÂü∫Ê®°ÂûãÊ∏ÖÂñÆÔºàÈÅøÂÖç clone ÊôÇÂÜçÈñã GPUÔºâ
        cpu_estimators = [(name, _as_cpu_estimator(est)) for name, est in self.base_estimators]

        # ============ ÂàÜÊîØ‰∏ÄÔºöVoting + Optuna ============ 
        if self.use_optuna and voting_mode in ("soft", "hard"):
            try:
                final_model, metrics, composition = self._optuna_voting_search(cpu_estimators, voting_mode)
                self._save_ensemble(final_model)
                self._write_final_txt(kind="voting", metrics=metrics, composition=composition)
                kind_desc = f"OptunaEnsemblerÔºàmode='{self.ens.get('MODE', 'free')}'Ôºâ"
                return self._finalize(kind_desc, final_model, metrics, composition)
            except Exception as e:
                print(f"‚ö†Ô∏è ÈõÜÊàê Optuna ÊêúÂ∞ãÂ§±ÊïóÔºåÊîπÁî®Âõ∫ÂÆöÂ≠êÈõÜÊêúÂ∞ãÔºö{e}")

        # ============ ÂàÜÊîØ‰∫åÔºöÂõ∫ÂÆöÂ≠êÈõÜÊûöËàâ ============ 
        if voting_mode in ("soft", "hard"):
            if self.ens.get("SEARCH", "none") == "voting_subsets":
                print("üîç ÈõÜÊàêÊêúÂ∞ãÔºö‰ΩøÁî®Âõ∫ÂÆöÂ≠êÈõÜÊûöËàâÊêúÂ∞ãÔºàTop-KÔºâ„ÄÇ")
                results = self._search_voting_subsets(cpu_estimators, voting_mode)
                # ‰ª•ÊúÄ‰Ω≥ÂêçÁ®±ÁµÑÂêàÈáçË®ì‰∏¶‰øùÂ≠òÔºàÈÅøÂÖçÊää estimator Áâ©‰ª∂ÂØ´ÂÖ• JSONÔºâ
                best_names = results["best_names"]
                pool = {n: e for n, e in cpu_estimators}
                best_ests = [(n, clone(pool[n])) for n in best_names]

                final_model = self._fit_voting(best_ests, voting_mode)
                self._save_ensemble(final_model)

                # ‰øùÂ≠òÂ†±ÂëäÔºàjson ÂÖ®ÂèØÂ∫èÂàóÂåñ + txtÔºâ
                self._dump_json(
                    os.path.join(self.out_dir, "reports", "ensemble_search_report.json"),
                    self._np_to_py({
                        "mode": results["mode"],
                        "topk": results["topk"],
                        "all_evaluated": results["all_evaluated"]
                    })
                )
                self._write_topk_txt(results["topk"])

                metrics, composition = self._eval_and_compose(final_model, kind="voting", estimators=best_ests)
                self._write_final_txt(kind="voting", metrics=metrics, composition=composition)
                kind_desc = f"VotingClassifierÔºàvoting='{voting_mode}'Ôºâ"
                return self._finalize(kind_desc, final_model, metrics, composition)
            else:
                final_model = self._fit_voting(cpu_estimators, voting_mode)
                self._save_ensemble(final_model)
                metrics, composition = self._eval_and_compose(final_model, kind="voting", estimators=cpu_estimators)
                self._write_final_txt(kind="voting", metrics=metrics, composition=composition)
                kind_desc = f"VotingClassifierÔºàvoting='{voting_mode}'Ôºâ"
                return self._finalize(kind_desc, final_model, metrics, composition)
        else:
            # Stacking
            final_model = self._fit_stacking(cpu_estimators, cv=stack_cv)
            self._save_ensemble(final_model)
            metrics, composition = self._eval_and_compose(final_model, kind="stacking", estimators=cpu_estimators)
            self._write_final_txt(kind="stacking", metrics=metrics, composition=composition)
            kind_desc = f"StackingClassifierÔºàcv={stack_cv}Ôºâ"
            return self._finalize(kind_desc, final_model, metrics, composition)

    # ============ Optuna ÊêúÂ∞ãÔºàVoting Â≠êÈõÜÔºãÊ¨äÈáçÔºâ ============
    def _optuna_voting_search(self, estimators: List[Tuple[str, Any]], voting_mode: str):
        """‰ΩøÁî® OptunaEnsembler ÊêúÂ∞ãÊúÄ‰Ω≥Ê¨äÈáçËàáÂ≠êÈõÜ„ÄÇ"""
        est_dict = {n: e for n, e in estimators}
        ens = OptunaEnsembler(
            task_type=self.task_type,
            n_splits=int(self.ens.get("STACK_CV", 5)),
            n_trials=int(self.ens.get("OPTUNA_TRIALS", 30)),
            weight_mode=self.ens.get("WEIGHT_MODE", "dirichlet"),
            mode=self.ens.get("MODE", "free"),
            min_models=int(self.ens.get("MIN_MODELS", 2)),
            max_models=self.ens.get("MAX_MODELS"),
            pruning=bool(self.ens.get("PRUNING", True)),
            direction=self.ens.get("DIRECTION", "maximize"),
            seed=int(self.ens.get("SEED", 42)),
            report_dir=os.path.join(self.out_dir, "reports"),
        )
        info = ens.fit(est_dict, self.X_train, self.y_train, self.X_valid, self.y_valid)
        proba = ens.predict_proba(self.X_valid)
        y_pred = np.argmax(proba, axis=1)
        metrics = _compute_metrics(self.task_type, self.y_valid, y_pred, proba)
        composition = {
            "type": "voting",
            "voting": "soft",
            "estimators": info["selected"],
            "weights": info["weights"],
        }
        self._dump_json(
            os.path.join(self.out_dir, "reports", "ensemble_optuna_best.json"),
            {
                "selected": info["selected"],
                "weights": info["weights"],
                "best_score": info["best_score"],
                "strategy": info["strategy"],
                "metrics": self._np_to_py(metrics),
            },
        )
        return ens, metrics, composition

    # ---------------- Voting Â≠êÈõÜÊûöËàâÔºàÂõ∫ÂÆöÊêúÂ∞ãÔºâ ----------------
    def _search_voting_subsets(self, estimators: List[Tuple[str, Any]], mode: str) -> Dict[str, Any]:
        names = [n for n, _ in estimators]
        pool = {n: e for n, e in estimators}
        max_subset = int(self.ens.get("SEARCH_MAX_SUBSET", 4))
        topk_n = int(self.ens.get("SEARCH_TOPK", 3))

        cand_sets: List[List[str]] = []
        for r in range(2, min(max_subset, len(names)) + 1):
            for combo in itertools.combinations(names, r):
                cand_sets.append(list(combo))

        results_serializable = []
        best_tuple = None  # (metrics_key, names)
        for comb in cand_sets:
            ests = [(n, clone(pool[n])) for n in comb]
            model = self._fit_voting(ests, mode)
            metrics, _ = self._eval_and_compose(model, kind="voting", estimators=ests)

            # ÂèØÂ∫èÂàóÂåñÔºöÂè™Â≠ò names/metrics
            results_serializable.append({
                "names": list(comb),
                "metrics": self._np_to_py(metrics),
            })

            # ÊéíÂ∫èÈçµÔºàmacro-F1ÔºåÂÖ∂Ê¨° ACC„ÄÅAUCÔºâ
            auc_val = metrics.get("auc", np.nan)
            key = (metrics["f1"], metrics["acc"], -np.nan_to_num(auc_val, nan=-1.0))
            if (best_tuple is None) or (key > best_tuple[0]):
                best_tuple = (key, list(comb))

        # ‰æùÁõÆÊ®ôÊéíÂ∫èÔºåÂèñ Top-KÔºàÂ∑≤ÊòØÂèØÂ∫èÂàóÂåñÁµêÊßãÔºâ
        def _key(r):
            m = r["metrics"]
            return (m["f1"], m["acc"], -np.nan_to_num(m.get("auc", np.nan), nan=-1.0))
        results_serializable.sort(key=_key, reverse=True)
        topk = results_serializable[:topk_n]

        return {
            "mode": mode,
            "topk": topk,                         # ÂèØÂ∫èÂàóÂåñ
            "best_names": best_tuple[1],          # Êèê‰æõÊúÄ‰Ω≥ÂêçÁ®±Ê∏ÖÂñÆ‰ª•Âà©ÈáçË®ì
            "all_evaluated": len(results_serializable)
        }

    # ---------------- Âª∫Ê®°ËàáË©ï‰º∞ ----------------
    def _fit_voting(self, estimators: List[Tuple[str, Any]], mode: str):
        model = VotingClassifier(estimators=estimators, voting=mode, n_jobs=None)
        with parallel_backend("threading", n_jobs=1):
            model.fit(self.X_train, self.y_train)
        return model

    def _fit_stacking(self, estimators: List[Tuple[str, Any]], cv: int = 5):
        skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=42)
        final_est = LogisticRegression(max_iter=1000, n_jobs=1, multi_class="auto")
        model = StackingClassifier(estimators=estimators, final_estimator=final_est, cv=skf, n_jobs=None, passthrough=False)
        with parallel_backend("threading", n_jobs=1):
            model.fit(self.X_train, self.y_train)
        return model

    def _eval_and_compose(self, model, kind: str, estimators: List[Tuple[str, Any]]):
        y_pred = model.predict(self.X_valid)
        proba = None
        if hasattr(model, "predict_proba"):
            try:
                proba = model.predict_proba(self.X_valid)
            except Exception:
                proba = None
        metrics = _compute_metrics(self.task_type, self.y_valid, y_pred, proba)
        composition = self._extract_composition(model, kind, estimators)
        return metrics, composition

    def _extract_composition(self, model, kind: str, estimators: List[Tuple[str, Any]]) -> Dict[str, Any]:
        if kind == "voting" or isinstance(model, VotingClassifier):
            names = [n for n, _ in estimators]
            return {
                "type": "voting",
                "estimators": names,                          # ÂêçÁ®±Ê∏ÖÂñÆÔºàÂèØÂ∫èÂàóÂåñÔºâ
                "voting": getattr(model, "voting", "soft"),
                "weights": getattr(model, "weights", None),   # ÂèØËÉΩÁÇ∫ None ‚Üí Âπ≥Âùá
            }
        # stackingÔºöÂàóÂá∫ meta LR ‰øÇÊï∏ top-k
        names = [n for n, _ in estimators]
        n_classes = len(self.classes_)
        feat_names = []
        for n in names:
            for k in range(n_classes):
                feat_names.append(f"{n}::class_{k}")
        clf = model.final_estimator_
        coef = getattr(clf, "coef_", None)
        topk_map = {}
        if coef is not None:
            K = min(5, len(feat_names))
            for c_idx in range(coef.shape[0]):
                coefs = coef[c_idx, :]
                order = np.argsort(-np.abs(coefs))[:K]
                tops = [(feat_names[j], float(coefs[j])) for j in order]
                cls_label = self.classes_[c_idx] if c_idx < len(self.classes_) else f"idx_{c_idx}"
                topk_map[cls_label] = tops
        return {
            "type": "stacking",
            "feature_names": feat_names,
            "coef": coef if coef is None else coef.tolist(),
            "classes": self.classes_.tolist(),
            "topk_per_class": topk_map
        }

    # ---------------- ‰øùÂ≠ò ----------------
    def _save_ensemble(self, model):
        try:
            os.makedirs(os.path.join(self.out_dir, "models"), exist_ok=True)
            path = os.path.join(self.out_dir, "models", "ensemble_best.joblib")
            dump(model, path)
            print(f"üíæ Â∑≤‰øùÂ≠òÊúÄ‰Ω≥ EnsembleÔºö{path}")
        except Exception as e:
            print(f"‚ö†Ô∏è ‰øùÂ≠ò Ensemble Â§±ÊïóÔºö{e}")

    def _write_topk_txt(self, topk: List[Dict[str, Any]]) -> None:
        """Â∞á Voting Â≠êÈõÜÊêúÂ∞ãÁöÑÂâç K ÂêçËº∏Âá∫ÁÇ∫Á¥îÊñáÂ≠óÊëòË¶Å„ÄÇ"""
        lines = []
        lines.append("üèÜ Ensemble Voting ÁµÑÂêà Top-KÔºö")
        for i, r in enumerate(topk, 1):
            m = r["metrics"]
            auc_val = m.get("auc", np.nan)
            auc_str = "nan" if np.isnan(auc_val) else f"{auc_val:.6f}"
            lines.append(f"[{i}] {list(r['names'])} | ACC={m['acc']:.6f} | macro-F1={m['f1']:.6f} | AUC={auc_str}")
        p = os.path.join(self.out_dir, "reports", "ensemble_topk.txt")
        os.makedirs(os.path.dirname(p), exist_ok=True)
        with open(p, "w", encoding="utf-8") as f:
            f.write("\n".join(lines))

    def _write_final_txt(self, kind: str, metrics: Dict[str, Any], composition: Dict[str, Any]) -> None:
        """Ëº∏Âá∫ÊúÄÁµÇÈÅ∏Áî®ÁöÑ Ensemble ÁµÑÊàêËàáÊàêÁ∏æÔºàÁ¥îÊñáÂ≠óÔºâ„ÄÇ"""
        auc_val = metrics.get("auc", np.nan)
        auc_str = "nan" if np.isnan(auc_val) else f"{auc_val:.6f}"
        lines = []
        lines.append("üß© Ensemble ÊúÄÁµÇÈÅ∏Áî®Ôºö")
        if kind == "voting" or composition.get("type") == "voting":
            lines.append(f"  È°ûÂûãÔºöVotingÔºàvoting='{composition.get('voting', 'soft')}'Ôºâ")
            lines.append(f"  Âü∫Ê®°ÂûãÔºö{composition.get('estimators')}")
            lines.append(f"  Ê¨äÈáçÔºö{composition.get('weights') if composition.get('weights') is not None else 'Âπ≥ÂùáÔºàNoneÔºâ'}")
        else:
            lines.append("  È°ûÂûãÔºöStackingÔºàmeta-learner = LogisticRegressionÔºâ")
        lines.append(f"  ÊàêÁ∏æÔºöACC={metrics['acc']:.6f} | F1={metrics['f1']:.6f} | AUC={auc_str}")
        p = os.path.join(self.out_dir, "reports", "ensemble_final.txt")
        os.makedirs(os.path.dirname(p), exist_ok=True)
        with open(p, "w", encoding="utf-8") as f:
            f.write("\n".join(lines))

    # ---------------- Â∑•ÂÖ∑ ----------------
    def _dump_json(self, path: str, obj: Any):
        os.makedirs(os.path.dirname(path), exist_ok=True)
        with open(path, "w", encoding="utf-8") as f:
            json.dump(obj, f, ensure_ascii=False, indent=2)

    def _np_to_py(self, d: Dict[str, Any]) -> Dict[str, Any]:
        def _conv(v):
            import numpy as np
            if isinstance(v, (np.floating, np.integer)): return v.item()
            if isinstance(v, np.ndarray): return v.tolist()
            return v
        return {k: _conv(v) for k, v in d.items()}

    def _finalize(self, kind_desc: str, final_model, metrics: Dict[str, Any], composition: Dict[str, Any]) -> Dict[str, Any]:
        print(f"üß± ‰ΩøÁî® {kind_desc}")
        print("üß™ È©óË≠â Ensemble ‰∏≠ ...")
        auc_val = metrics.get("auc", np.nan)
        auc_str = "nan" if np.isnan(auc_val) else f"{auc_val:.6f}"
        if self.task_type == "binary":
            print(f"üìà AUC={auc_str} | ACC={metrics['acc']:.6f} | F1={metrics['f1']:.6f}")
            print(f"‚úÖ Ensemble ÂÆåÊàêÔΩúAUC={auc_str} | ACC={metrics['acc']:.6f} | F1={metrics['f1']:.6f}")
            print("‚ÑπÔ∏è  ‰∫åÂÖÉÂàÜÈ°ûÂèØÁî® thresholdÔºàÈ†êË®≠ THRESHOLDÔºâÔºõË©≥Á¥∞Â†±Ë°®Ë¶ã‰∏ãÊñπ„ÄÇ")
        else:
            print(f"üìà OVR-AUC={auc_str} | ACC={metrics['acc']:.6f} | macro-F1={metrics['f1']:.6f}")
            print(f"‚úÖ Ensemble ÂÆåÊàêÔΩúAUC={auc_str} | ACC={metrics['acc']:.6f} | F1={metrics['f1']:.6f}")
            print("‚ÑπÔ∏è  Â§öÈ°ûÂà•‰∏çÂ•óÁî® thresholdÔºõË©≥Á¥∞Â†±Ë°®Ë¶ã‰∏ãÊñπ„ÄÇ")
        print("üìã ÂàÜÈ°ûÂ†±ÂëäÔºö")
        print(metrics["report"])
        print("üìù Ê∑∑Ê∑ÜÁü©Èô£Ôºö")
        print(metrics["confusion_matrix"])
        if metrics.get("pred_dist"):
            print(f"üì¶ È†êÊ∏¨È°ûÂà•ÂàÜ‰ΩàÔºö{metrics['pred_dist']}")
        print("üß© Ensemble ÁµÑÊàêÔºö")
        if composition["type"] == "voting":
            print(f"  È°ûÂûãÔºöVotingÔºàvoting='{composition['voting']}'Ôºâ")
            print(f"  Âü∫Ê®°ÂûãÔºö{composition['estimators']}")
            print(f"  Ê¨äÈáçÔºö{composition['weights'] if composition['weights'] is not None else 'Âπ≥ÂùáÔºàNoneÔºâ'}")
        else:
            print("  È°ûÂûãÔºöStackingÔºàmeta-learner = LogisticRegressionÔºâ")
            topk = composition.get("topk_per_class", {})
            for cls_label, tops in topk.items():
                print(f"    - È°ûÂà• {cls_label}Ôºö")
                for feat, coef in tops:
                    print(f"        {feat:>24s} ‰øÇÊï∏={coef:+.4f}")
        return {
            "model": final_model,
            "metrics": metrics,
            "settings": self.ens,
            "composition": composition,
        }

    def _print_topk(self, topk: List[Dict[str, Any]]):
        print("\nüèÜ Ensemble Voting ÁµÑÂêà Top-3Ôºö")
        for i, r in enumerate(topk, 1):
            m = r["metrics"]
            auc_val = m.get("auc", np.nan)
            auc_str = "nan" if np.isnan(auc_val) else f"{auc_val:.6f}"
            print(f"[{i}] {list(r['names'])} | ACC={m['acc']:.6f} | macro-F1={m['f1']:.6f} | AUC={auc_str}")


# ---------------------------------------------------------------------------
# Lightweight wrapper for OptunaEnsembler
# ---------------------------------------------------------------------------


def run_ensemble(
    estimators: Dict[str, Any],
    X,
    y,
    X_valid=None,
    y_valid=None,
    cfg: Optional[Dict[str, Any]] = None,
):
    """Convenience wrapper around :class:`OptunaEnsembler`.

    Parameters
    ----------
    estimators: Dict[str, Any]
        Mapping from estimator name to unfitted estimator instances.
    X, y: array-like
        Training data and labels.
    X_valid, y_valid: array-like, optional
        Hold-out validation set. When provided, scores mix CV and validation
        results (0.3 * CV + 0.7 * Valid).
    cfg: Dict[str, Any], optional
        Configuration dictionary. Keys mirror ``OptunaEnsembler`` init
        arguments.
    """

    cfg = cfg or {}
    mode = cfg.get("MODE", "free")
    ens = OptunaEnsembler(
        task_type=cfg.get("TASK_TYPE", "binary"),
        n_splits=cfg.get("STACK_CV", 5),
        n_trials=cfg.get("OPTUNA_TRIALS", 50),
        weight_mode=cfg.get("WEIGHT_MODE", "dirichlet"),
        mode=mode,
        min_models=cfg.get("MIN_MODELS", 2),
        max_models=cfg.get("MAX_MODELS", None),
        pruning=cfg.get("PRUNING", True),
        direction=cfg.get("DIRECTION", "maximize"),
        seed=cfg.get("SEED", 42),
        report_dir=cfg.get("REPORT_DIR", "./reports"),
    )
    return ens.fit(estimators, X, y, X_valid, y_valid)
