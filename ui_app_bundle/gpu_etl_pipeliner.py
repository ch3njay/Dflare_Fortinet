# -*- coding: utf-8 -*-
"""
GPU ETL Pipelineï¼šæ¸…æ´— â†’ æ˜ å°„ â†’ ç‰¹å¾µå·¥ç¨‹ï¼ˆæ§åˆ¶å™¨ï¼Œæ‰€æœ‰äº’å‹•ä¸€æ¬¡å®Œæˆç‰ˆï¼‰

ç›®æ¨™ï¼š
- é•·æ™‚é–“åŸ·è¡Œæ™‚ï¼Œæ‰€æœ‰ã€Œéœ€è¦ä½¿ç”¨è€…é¸æ“‡ã€çš„å°è©±ï¼Œåœ¨é–‹è·‘å‰ä¸€æ¬¡åšå®Œã€‚
- æ¸…æ´—äº¤çµ¦ log_cleaning.clean_logs()ï¼ˆå…¶å…§å»ºæŠ½æ¨£äº’å‹•èˆ‡åŸå§‹è¼¸å…¥æª”é¸æ“‡ï¼‰ï¼Œä½†ä»åœ¨ã€Œé–‹å§‹è™•ç†ä¹‹å‰ã€å®Œæˆã€‚
- CLI ä¸€å¾‹ quiet=Falseï¼ˆé¡¯ç¤ºå›é¥‹ï¼‰ï¼›UI/èƒŒæ™¯å‘¼å« run_pipeline() æ™‚ quiet=Trueã€‚

äº’å‹•æ™‚æ©Ÿï¼ˆCLIï¼‰ï¼š
1) å•æ˜¯å¦åŸ·è¡Œå„éšæ®µï¼ˆæ¸…æ´—/æ˜ å°„/ç‰¹å¾µå·¥ç¨‹ï¼‰ï¼ŒBatch åƒæ•¸ã€‚
2) å• FE é¸é …ï¼ˆApprox / Top-K JSONï¼‰ï¼Œä¸¦**å…ˆå•æœ€çµ‚è¼¸å‡ºæª” out_path**ã€‚
3) è‹¥è·³éæ¸…æ´— â†’ å…ˆé¸ processed_logs.csvã€‚
   è‹¥è·³éæ˜ å°„ä½†åŸ·è¡Œ FE â†’ å…ˆé¸ preprocessed_data.csvã€‚
4) é–‹å§‹åŸ·è¡Œï¼šæ¸…æ´—ï¼ˆLC æœƒåœ¨é–‹è·‘å‰å®ŒæˆæŠ½æ¨£èˆ‡åŸå§‹è¼¸å…¥æª”é¸æ“‡ï¼‰ï¼Œæ˜ å°„ï¼Œç‰¹å¾µå·¥ç¨‹ï¼ˆç›´æ¥ç”¨å‰é¢å·²é¸ out_pathï¼Œä¸å†å½ˆçª—ï¼‰ã€‚

æ³¨æ„ï¼š
- è‹¥åœ¨ç„¡è¦–çª—ç’°å¢ƒåŸ·è¡Œï¼ˆä¾‹å¦‚ SSH æˆ–ç„¡ X serverï¼‰ï¼Œtkinter å°è©±æ¡†å°‡å¤±æ•—ï¼›æœ¬æª”å·²æä¾›è‡ªå‹•é™ç´šç‚º CLI è¼¸å…¥çš„ fallbackã€‚
"""

import os, time
import sys
from typing import Optional, Dict, Any, List, Tuple
from colorama import Fore, Style, init as colorama_init

colorama_init(autoreset=True)

# =============== Tk å°è©±æ¡†å·¥å…·ï¼ˆè‡ªå‹•é™ç´šï¼‰ ===============
def _has_tk() -> bool:
    try:
        import tkinter as tk  # noqa
        return True
    except Exception:
        return False

def _pick_files(title="é¸æ“‡è¼¸å…¥æª”æ¡ˆ", multiple=True,
                patterns=(("Log/CSV Files", "*.csv *.txt *.gz"),
                          ("All files", "*.*"))) -> List[str] | str:
    """
    é¦–é¸ï¼štk å°è©±æ¡†ï¼›å¤±æ•—å‰‡é™ç´šåˆ° CLI è¼¸å…¥ï¼ˆé¿å… headless crashï¼‰ã€‚
    multiple=True å›å‚³ List[str]ï¼›multiple=False å›å‚³ strï¼ˆæˆ– ""ï¼‰
    """
    if _has_tk():
        try:
            import tkinter as tk
            from tkinter import filedialog
            root = tk.Tk()
            root.withdraw()
            root.attributes('-topmost', True)
            root.update()
            try:
                if multiple:
                    paths = filedialog.askopenfilenames(title=title, filetypes=list(patterns))
                    return list(paths) if paths else []
                else:
                    path = filedialog.askopenfilename(title=title, filetypes=list(patterns))
                    return path or ""
            finally:
                root.destroy()
        except Exception:
            pass  # è½‰ç‚º CLI æ¨¡å¼

    # CLI é™ç´š
    print(Fore.YELLOW + f"âš ï¸ ç„¡æ³•ä½¿ç”¨åœ–å½¢é¸æª”ï¼Œæ”¹ç”¨ CLI è¼¸å…¥ â†’ {title}")
    if multiple:
        s = input(Fore.CYAN + "è«‹è¼¸å…¥è·¯å¾‘ï¼ˆå¤šæª”ä»¥åˆ†è™Ÿ ; åˆ†éš”ï¼‰ï¼š").strip()
        paths = [p.strip() for p in s.split(";") if p.strip()]
        return [p for p in paths if os.path.isfile(p)]
    else:
        p = input(Fore.CYAN + "è«‹è¼¸å…¥æª”æ¡ˆè·¯å¾‘ï¼š").strip()
        return p if os.path.isfile(p) else ""

def _save_file(title="é¸æ“‡è¼¸å‡ºæª”æ¡ˆ", default="engineered_data.csv") -> str:
    if _has_tk():
        try:
            import tkinter as tk
            from tkinter import filedialog
            root = tk.Tk()
            root.withdraw()
            root.attributes('-topmost', True)
            root.update()
            try:
                path = filedialog.asksaveasfilename(
                    title=title, initialfile=default, defaultextension=".csv",
                    filetypes=[("CSV files", "*.csv")]
                )
                return path or default
            finally:
                root.destroy()
        except Exception:
            pass
    # CLI é™ç´š
    s = input(Fore.CYAN + f"{title}ï¼ˆé è¨­ {default}ï¼‰ï¼š").strip()
    return s or default

def _pick_optional_json(title) -> Optional[str]:
    patterns = (("JSON files", "*.json"), ("All files", "*.*"))
    p = _pick_files(title=title, multiple=False, patterns=patterns)
    return p or None

# === åŒ¯å…¥å­æ¨¡çµ„ï¼ˆå„ªå…ˆ packageï¼Œå¤±æ•—å‰‡ localï¼‰ ===
try:
    from gpu_etl_pipeline.log_cleaning import clean_logs as LC
    from gpu_etl_pipeline.log_mapping import main as LM
    from gpu_etl_pipeline.feature_engineering import main as FE
except ImportError:
    cur_dir = os.path.dirname(os.path.abspath(__file__))
    if cur_dir not in sys.path:
        sys.path.append(cur_dir)
    from log_cleaning import clean_logs as LC
    from log_mapping import main as LM
    from feature_engineering import main as FE

# =============== å°å·¥å…·ï¼šå•ç­”/æª¢æŸ¥/ä¿åº• ===============
def _ask_yn_10(msg: str, default_true: bool) -> bool:
    """1/0 å•ç­”ï¼›Enter å¥—é è¨­"""
    while True:
        s = input(Fore.CYAN + f"{msg} (1=æ˜¯,0=å¦ï¼›é è¨­ {1 if default_true else 0})ï¼š").strip()
        if s == "": return default_true
        if s in ("0","1"): return s == "1"
        print(Fore.RED + "âŒ è¼¸å…¥éŒ¯èª¤ï¼Œè«‹é‡æ–°è¼¸å…¥ï¼")

def _ask_int(msg: str, default: int) -> int:
    s = input(Fore.CYAN + f"{msg}ï¼ˆé è¨­ {default}ï¼‰ï¼š").strip()
    try:
        return int(s) if s else default
    except Exception:
        return default

def _check_file_exists(path: Optional[str], module_name: str, allow_small=False) -> str:
    """#DEBUG æª¢æŸ¥ï¼šæª”æ¡ˆå­˜åœ¨ï¼›å¿…è¦æ™‚æª¢æŸ¥éç©º"""
    if not path:
        raise RuntimeError(f"âŒ [{module_name}] æœªå–å¾—è¼¸å‡ºæª”è·¯å¾‘")
    if not os.path.isfile(path):
        raise RuntimeError(f"âŒ [{module_name}] æ‰¾ä¸åˆ°è¼¸å‡ºæª”ï¼š{path}")
    if (not allow_small) and os.path.getsize(path) < 100:
        raise RuntimeError(f"âŒ [{module_name}] è¼¸å‡ºæª”éå°æˆ–ç‚ºç©ºï¼š{path}")
    print(Fore.GREEN + f"[DEBUG] {module_name} è¼¸å‡ºæª”æª¢æŸ¥é€šéï¼š{path}")
    return path

def _coalesce_return(ret_value: Optional[str], fallback: str, module_name: str) -> str:
    """
    æŸäº› module.main() å¯èƒ½ä¸å›å‚³è·¯å¾‘ï¼›ä½¿ç”¨ fallbackï¼ˆæˆ‘å€‘å‚³å…¥çš„ out_csvï¼‰
    å†åšå­˜åœ¨æª¢æŸ¥ï¼Œé¿å… silent failã€‚
    """
    path = ret_value or fallback
    return _check_file_exists(path, module_name)

# =============== æ ¸å¿ƒ Pipelineï¼ˆä¾› UI / è‡ªå‹•åŒ–å‘¼å«ï¼‰ ===============
def run_pipeline(
    in_paths: Optional[List[str] | str],
    out_path: str = "engineered_data.csv",
    do_clean: bool = True,
    do_map: bool = True,
    do_fe: bool = True,
    batch_mode: bool = False,
    batch_size: int = 50_000,
    sampling_config: Optional[Dict[str, Any]] = None,  # e.g. {"method":"random","ratio":0.1,"label_col":"is_attack","seed":42}
    fe_enable=None,
    topk_src_port_json: Optional[str] = None,
    topk_pair_json: Optional[str] = None,
    approx_mode: bool = False,
    quiet: bool = True
) -> str:
    """
    å»ºè­° UI/èƒŒæ™¯å‘¼å«ï¼ˆquiet=Trueï¼‰ã€‚
    do_clean=Trueï¼šè‹¥ quiet=Trueï¼Œæœƒä»¥éœé»˜åƒæ•¸å‘¼å« LCï¼ˆéœ€æä¾› pathsï¼‰ï¼›quiet=False å‰‡äº¤çµ¦ LC è‡ªè¡Œäº’å‹•ã€‚
    do_clean=Falseï¼šin_paths æ‡‰æä¾› processed_logs.csvã€‚
    do_map=False ä½† do_fe=Trueï¼šin_paths æ‡‰æä¾› preprocessed_data.csvã€‚
    """
    # 1) æ­£å¸¸åŒ–è¼¸å…¥è·¯å¾‘
    if isinstance(in_paths, str):
        in_paths = [in_paths]
    paths_list = in_paths or []

    current_input: Optional[str] = None

    # 2) æ¸…æ´—
    if do_clean:
        if quiet:
            # UI/èƒŒæ™¯ï¼šéœé»˜å‘¼å« LCï¼ˆpaths å¯ç‚ºå¤šæª”ï¼›LC æœƒè™•ç†ï¼‰
            lc_kwargs = dict(
                quiet=True,
                paths=paths_list if paths_list else None,
                clean_csv="processed_logs.csv",
                enable_sampling=False
            )
            if sampling_config is not None:
                lc_kwargs["sampling_cfg"] = sampling_config
            lc_out = LC(**lc_kwargs)  # æœŸæœ›å›å‚³ processed_logs.csv
        else:
            # CLIï¼šäº¤çµ¦ LC è‡ªè¡Œäº’å‹•
            lc_out = LC(quiet=False, enable_sampling=True)
        current_input = _coalesce_return(lc_out, "processed_logs.csv", "Cleaning")
        print(Style.BRIGHT + Fore.GREEN + f"âœ… Cleaning å®Œæˆ â†’ {current_input}" + Style.RESET_ALL)
    else:
        # ç•¥éæ¸…æ´—ï¼šä½¿ç”¨å¤–éƒ¨å‚³å…¥çš„ processed_logs.csv
        if not paths_list:
            raise FileNotFoundError("ç•¥éæ¸…æ´—æ™‚ï¼Œéœ€æä¾› processed_logs.csv æˆ–ç­‰åƒ¹ CSV è·¯å¾‘")
        current_input = _check_file_exists(paths_list[0], "Cleaning(skip)", allow_small=True)

    # 3) æ˜ å°„
    if do_map:
        lm_out_path = "preprocessed_data.csv"
        lm_ret = LM(in_csv=current_input, out_csv=lm_out_path,
                    batch_mode=batch_mode, batch_size=batch_size, quiet=quiet)
        current_input = _coalesce_return(lm_ret, lm_out_path, "Mapping")
        if not quiet:
            print(Style.BRIGHT + Fore.GREEN + f"âœ… Mapping å®Œæˆ â†’ {current_input}" + Style.RESET_ALL)
    else:
        # ç•¥éæ˜ å°„ï¼šcurrent_input å¿…é ˆæ˜¯ FE å¯è®€çš„ CSVï¼ˆé€šå¸¸ç‚º preprocessed_data.csvï¼‰
        current_input = _check_file_exists(current_input, "Mapping(skip)", allow_small=False)

    # 4) ç‰¹å¾µå·¥ç¨‹
    if do_fe:
        fe_ret = FE(
            in_csv=current_input,
            out_csv=out_path,   # ä½¿ç”¨ä¸€é–‹å§‹å°±æ±ºå®šå¥½çš„ out_pathï¼Œä¸å†å½ˆçª—
            fe_enable=fe_enable,
            topk_src_port_json=topk_src_port_json,
            topk_pair_json=topk_pair_json,
            batch_mode=batch_mode,
            batch_size=batch_size,
            approx_mode=approx_mode,
            quiet=quiet
        )
        final_out = _coalesce_return(fe_ret, out_path, "FeatureEngineering")
        if not quiet:
            print(Style.BRIGHT + Fore.GREEN + f"âœ… Feature Engineering å®Œæˆ â†’ {final_out}" + Style.RESET_ALL)
        return final_out
    else:
        if not quiet:
            print(Fore.YELLOW + f"âš ï¸ å·²è·³éç‰¹å¾µå·¥ç¨‹ï¼›æœ€çµ‚è¼¸å‡ºç‚ºä¸Šä¸€éšæ®µæª”æ¡ˆï¼š{current_input}")
        return current_input  # type: ignore


# =============== äº’å‹•å¼æ§åˆ¶å™¨ï¼ˆCLIï¼›æ‰€æœ‰äº’å‹•å…ˆå•å®Œï¼‰ ===============
def run_pipeline_cli() -> None:
    print(Style.BRIGHT + "==== GPU ETL Pipeline æ§åˆ¶å™¨ï¼ˆCLIï¼‰====")

    # â˜… 0) å»ºç«‹æœ¬æ¬¡åŸ·è¡Œçš„ run_dirï¼ˆé›†ä¸­æ‰€æœ‰ç”¢ç‰©ï¼‰
    run_dir = os.path.abspath(f"./artifacts/{time.strftime('%Y%m%d_%H%M%S')}")
    os.makedirs(run_dir, exist_ok=True)
    print(Fore.CYAN + f"[INFO] æœ¬æ¬¡ artifacts ç›®éŒ„ï¼š{run_dir}")

    # 1) éšæ®µé¸æ“‡ï¼ˆä¸€æ¬¡å•å®Œï¼‰
    do_clean = _ask_yn_10("æ˜¯å¦åŸ·è¡Œ æ¸…æ´—/æ¨™æº–åŒ–ï¼ˆlog_cleaningï¼‰", True)
    do_map   = _ask_yn_10("æ˜¯å¦åŸ·è¡Œ å­—å…¸æ˜ å°„/æ’åºï¼ˆlog_mappingï¼‰", True)
    do_fe    = _ask_yn_10("æ˜¯å¦åŸ·è¡Œ ç‰¹å¾µå·¥ç¨‹ï¼ˆfeature_engineeringï¼‰", True)

    # 2) Batch åƒæ•¸ï¼ˆä¸€æ¬¡å•å®Œï¼‰
    batch_mode = _ask_yn_10("æ˜¯å¦å•Ÿç”¨ Batch Mode", False)
    batch_size = _ask_int("batch_size", 50_000) if batch_mode else 50_000

    # 3) ç‰¹å¾µå·¥ç¨‹é¸é …ï¼ˆé–‹å§‹å‰å°±ä¸€æ¬¡é¸å®Œï¼‰
    approx_mode: bool = False
    topk_src: Optional[str] = None
    topk_pair: Optional[str] = None
    out_csv: str = "engineered_data.csv"

    if do_fe:
        approx_mode = _ask_yn_10("å•Ÿç”¨ Approx æ¨¡å¼ï¼ˆåƒ…æ‰¹å…§è¿‘ä¼¼çµ±è¨ˆï¼‰", False)
        if _ask_yn_10("æŒ‡å®š Top-K SrcPort JSONï¼Ÿ", False):
            topk_src = _pick_optional_json("é¸æ“‡ Top-K Src Port JSON") or None
        if _ask_yn_10("æŒ‡å®š Top-K Pair JSONï¼Ÿ", False):
            topk_pair = _pick_optional_json("é¸æ“‡ Top-K Pair JSON") or None
        print("ğŸ’¾ è«‹é¸æ“‡æœ€çµ‚è¼¸å‡ºæª”ï¼ˆengineered_data.csvï¼‰")
        user_pick = _save_file(title="é¸æ“‡æœ€çµ‚è¼¸å‡ºæª”", default="engineered_data.csv")
        # â˜… 3.1 è‹¥ä½¿ç”¨è€…æ²’æ”¹åæˆ–é¸ç›¸å°è·¯å¾‘ï¼Œçµ±ä¸€è½åˆ° 02_fe/
        if os.path.isabs(user_pick):
            out_csv = user_pick
        else:
            out_csv = os.path.join(run_dir, "02_fe", os.path.basename(user_pick))
        os.makedirs(os.path.dirname(out_csv), exist_ok=True)

    # 4) å‰ç½®æª”æ¡ˆé¸æ“‡ï¼ˆè·³éæŸéšæ®µæ™‚ï¼Œå…ˆæŠŠè·¯å¾‘å•å¥½ï¼‰
    in_source: Optional[str] = None   # ä¾›æ˜ å°„çš„è¼¸å…¥ï¼ˆç•¥éæ¸…æ´—æ™‚ï¼‰
    fe_in_csv: Optional[str] = None   # ä¾› FE çš„è¼¸å…¥ï¼ˆç•¥éæ¸…æ´—èˆ‡æ˜ å°„æ™‚ï¼‰

    if not do_clean:
        if do_map:
            print("ğŸ“‚ å·²ç•¥éæ¸…æ´—ï¼Œè«‹é¸æ“‡ä½œç‚ºã€æ˜ å°„è¼¸å…¥ã€çš„ processed_logs.csvï¼ˆæˆ–ç­‰åƒ¹ CSVï¼‰")
            p = _pick_files(
                title="é¸æ“‡æ˜ å°„è¼¸å…¥ï¼ˆprocessed_logs.csv æˆ–ç­‰åƒ¹ï¼‰",
                multiple=False,
                patterns=(("CSV files", "*.csv"), ("All files", "*.*"))
            )
            in_source = p if isinstance(p, str) else (p[0] if p else "")
            if not in_source:
                print(Fore.RED + "âŒ æœªé¸æ“‡æ˜ å°„è¼¸å…¥ï¼Œæµç¨‹çµæŸ")
                return
        else:
            if do_fe:
                print("ğŸ“‚ å·²ç•¥éæ¸…æ´—èˆ‡æ˜ å°„ï¼Œè«‹é¸æ“‡ã€ç‰¹å¾µå·¥ç¨‹è¼¸å…¥ã€çš„ CSV")
                p = _pick_files(
                    title="é¸æ“‡ FE è¼¸å…¥ CSV",
                    multiple=False,
                    patterns=(("CSV files", "*.csv"), ("All files", "*.*"))
                )
                fe_in_csv = p if isinstance(p, str) else (p[0] if p else "")
                if not fe_in_csv:
                    print(Fore.RED + "âŒ æœªé¸æ“‡ FE è¼¸å…¥ï¼Œæµç¨‹çµæŸ")
                    return

    # 5) é–‹å§‹åŸ·è¡Œï¼ˆä¹‹å¾Œä¸å†å‡ºç¾ä»»ä½•å°è©±æ¡†ï¼‰
    print(Style.BRIGHT + f"ğŸš€ é–‹å§‹åŸ·è¡Œï¼ˆCLI æ¨¡å¼ quiet=Falseï¼›æ‰€æœ‰é¸é …å·²ä¸€æ¬¡è¨­å®šå®Œæˆï¼‰...")
    print(Fore.YELLOW + f"[DEBUG] do_clean={do_clean}, do_map={do_map}, do_fe={do_fe}, "
                        f"batch_mode={batch_mode}, batch_size={batch_size}, approx_mode={approx_mode}")
    if do_fe:
        print(Fore.YELLOW + f"[DEBUG] FE out_csv={out_csv}, topk_src={topk_src}, topk_pair={topk_pair}")

    # 5.1 æ¸…æ´—
    if do_clean:
        # â˜… 5.1-a å‚³å…¥ run_dirï¼›è®“ LC æŠŠ processed/sample è½åœ¨ 00_clean/ï¼Œä¸¦å¯« manifest.clean.active_clean_file
        cleaned_csv = LC(quiet=False, enable_sampling=True, run_dir=run_dir)
        current_input = _coalesce_return(cleaned_csv, "processed_logs.csv", "Cleaning")
    else:
        current_input = in_source or fe_in_csv  # å…©è€…åªæœƒæœ‰ä¸€å€‹è¢«è³¦å€¼
        current_input = _check_file_exists(current_input, "Cleaning(skip)", allow_small=True)

    # 5.2 æ˜ å°„
    if do_map:
        # â˜… 5.2-a å‡ºåŠ›å›ºå®šåˆ° run_dir/01_map/
        lm_out_path = os.path.join(run_dir, "01_map", "preprocessed_data.csv")
        os.makedirs(os.path.dirname(lm_out_path), exist_ok=True)

        # â˜… 5.2-b å‚³ run_dir + use_manifest=True â†’ LM æœƒå„ªå…ˆåƒ manifest.clean.active_clean_fileï¼ˆæŠ½æ¨£å„ªå…ˆï¼‰
        lm_ret = LM(
            in_csv=current_input,
            out_csv=lm_out_path,
            batch_mode=batch_mode,
            batch_size=batch_size,
            quiet=False,
            run_dir=run_dir,          # â˜…
            use_manifest=True         # â˜…
        )
        current_input = _coalesce_return(lm_ret, lm_out_path, "Mapping")
        print(Fore.GREEN + f"âœ… Mapping å®Œæˆ â†’ {current_input}" + Style.RESET_ALL)
    else:
        if do_fe:
            print(Fore.YELLOW + f"âš ï¸ å·²è·³éæ˜ å°„ï¼›è¼¸å…¥çµ¦ FEï¼š{current_input}")

    # 5.3 ç‰¹å¾µå·¥ç¨‹
    if do_fe:
        # â˜… 5.3-a è‹¥æ˜ å°„æœ‰åŸ·è¡Œ â†’ FE å¯ç”¨ manifest.map.outputï¼›å¦å‰‡å°±ç”¨ä½¿ç”¨è€…é¸çš„ fe_in_csv
        use_manifest_for_fe = bool(do_map)

        fe_ret = FE(
            in_csv=current_input,
            out_csv=out_csv,
            fe_enable=None,
            topk_src_port_json=topk_src,
            topk_pair_json=topk_pair,
            batch_mode=batch_mode,
            batch_size=batch_size,
            approx_mode=approx_mode,
            quiet=False,
            run_dir=run_dir,                  # â˜… è®“ FE èƒ½å¯«å…¥ 02_fe ä¸¦æ›´æ–° manifest.fe
            use_manifest=use_manifest_for_fe  # â˜… æ˜ å°„æœ‰è·‘æ‰ç”¨ manifest çš„ map.output
        )
        final_out = _coalesce_return(fe_ret, out_csv, "FeatureEngineering")
        print(Style.BRIGHT + Fore.GREEN + f"âœ… Feature Engineering å®Œæˆ â†’ {final_out}" + Style.RESET_ALL)
        print(Style.BRIGHT + Fore.YELLOW + f"ğŸ‰ Pipeline å…¨éƒ¨å®Œæˆï¼Œæœ€çµ‚è¼¸å‡ºï¼š{final_out}" + Style.RESET_ALL)
    else:
        print(Style.BRIGHT + Fore.YELLOW + f"ğŸ‰ Pipeline å®Œæˆï¼ˆFE è·³éï¼‰ï¼Œæœ€çµ‚è¼¸å‡ºï¼š{current_input}" + Style.RESET_ALL)
# =============== å…¥å£ ===============
if __name__ == "__main__":
    run_pipeline_cli()
