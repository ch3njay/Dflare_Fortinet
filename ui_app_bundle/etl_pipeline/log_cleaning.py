# -*- coding: utf-8 -*-
"""
log_cleaning.py
- åˆ†å¡Šæµå¼æ¸…æ´—ï¼ˆTB ç­‰ç´šã€é˜² OOMï¼‰
- é€²åº¦æ¢ï¼ˆtqdmï¼‰ã€è‰²å½©è¼¸å‡ºï¼ˆcoloramaï¼‰
- CLI äº’å‹•åƒ…åœ¨ __main__ï¼›å¯ç”¨ QUIET/åƒæ•¸éœé»˜åŸ·è¡Œï¼ˆä¾› pipeline/UI å‘¼å«ï¼‰
- ä¸»è³‡æ–™ï¼šç§»é™¤ raw_logï¼›ä¿ç•™ idseq ä¸¦ç½®ç¬¬ä¸€æ¬„
- å³æ™‚é›™æª”è¼¸å‡ºï¼ˆæœªæŠ½æ¨£ + æŠ½æ¨£ï¼‰
- å”¯ä¸€å€¼æ¸…å–®ï¼ˆjson/txtï¼‰
"""
import os, re, gzip, json, time, logging
import pandas as pd, numpy as np
from tqdm import tqdm
from colorama import init, Fore, Style
from .utils import check_and_flush

# å¯éœé»˜çš„å…¨åŸŸæ——æ¨™ï¼ˆé è¨­ Falseï¼›ç”±å¤–éƒ¨è¨­å®š True å¯é—œé–‰æ‰€æœ‰è¼¸å‡ºèˆ‡äº’å‹•ï¼‰
QUIET = False

# å¯é¸ï¼šGUIï¼›ä¸å¯ç”¨æ™‚è‡ªå‹•é€€åŒ–ç‚º CLI è¼¸å…¥ï¼ˆåœ¨ QUIET æ¨¡å¼ä¸‹ä¸å•Ÿç”¨ GUIï¼‰
try:
    import tkinter as tk
    from tkinter import filedialog
    TK_OK = True
except Exception:
    TK_OK = False

# å¯é¸ï¼šè‡ªå‹•åµæ¸¬ç·¨ç¢¼
try:
    import chardet
    HAS_CHARDET = True
except Exception:
    HAS_CHARDET = False

init(autoreset=True)
logging.basicConfig(filename="log_cleaning_error.log", level=logging.ERROR,
                    format="%(asctime)s - %(levelname)s - %(message)s")

# =====================[ CONFIG ]=====================
CHUNK_LINES = 50_000
DEFAULT_SAMPLING_SEED = 42
DEFAULT_RANDOM_RATIO = 1.0
DEFAULT_WRITE_RAWDICT = False   # å¦‚éœ€ idseqâ†’raw_log å¤–æ›å­—å…¸ï¼Œè¨­ Trueï¼ˆå£“ç¸® JSONLï¼‰
RAWDICT_GZ_PATH = "rawlog_dict.jsonl.gz"

# æ¬„ä½é †åºï¼ˆæ ¸å¿ƒè¼¸å‡ºï¼›ç¬¬ä¸€æ¬„ idseqï¼›ç„¡ raw_logï¼‰
COLUMN_ORDER = [
    'idseq', 'datetime', 'subtype',
    'srcip','srcport','srcintf',
    'dstip','dstport','dstintf',
    'action','sentpkt','rcvdpkt',
    'duration','service','devtype','level',
    'crscore','crlevel','is_attack'
]

UNIQUE_COLS = ["subtype", "level", "srcintf", "dstintf", "action", "service", "devtype", "crlevel"]
# ====================================================

KV_PATTERN = re.compile(r'(\w+)=(".*?"|\'.*?\'|[^"\',\s]+)')

# -------------------- å·¥å…· --------------------
def _get_tk_root():
    root = tk.Tk(); root.withdraw(); return root

def _select_files_interactive():
    # åƒ…åœ¨é QUIET æ‰å¯èƒ½å•Ÿå‹• GUI/CLI è¼¸å…¥
    if (not QUIET) and TK_OK:
        print(f"{Fore.WHITE}ã€æç¤ºã€‘ğŸ“‚ é¡¯ç¤ºæª”æ¡ˆé¸æ“‡ï¼ˆå¯å¤šé¸ï¼‰")
        _get_tk_root()
        paths = filedialog.askopenfilenames(
            title="é¸æ“‡æ—¥èªŒæª”æ¡ˆ",
            filetypes=[("Log/CSV files", "*.txt *.csv *.gz"), ("All files","*.*")]
        )
        if paths: return list(paths)
    while not QUIET:
        s = input("è«‹è¼¸å…¥æ—¥èªŒæª”è·¯å¾‘ï¼ˆå¯å¤šå€‹ï¼Œåˆ†è™Ÿ;åˆ†éš”ï¼‰ï¼š").strip()
        if s:
            paths = [p.strip() for p in s.split(";") if p.strip()]
            if all(os.path.exists(p) for p in paths):
                return paths
        print("âŒ è·¯å¾‘éŒ¯èª¤ï¼Œè«‹é‡è©¦ã€‚")
    # QUIET æ¨¡å¼èµ°åˆ°é€™ä»£è¡¨å¤–éƒ¨æœªæä¾› paths
    raise ValueError("QUIET æ¨¡å¼éœ€ç”±å‘¼å«ç«¯æä¾› paths åƒæ•¸ã€‚")

def _select_save_path_interactive(prompt, default_name):
    # åƒ…åœ¨é QUIET æ‰å¯èƒ½å•Ÿå‹• GUI/CLI è¼¸å…¥
    if (not QUIET) and TK_OK:
        _get_tk_root()
        p = filedialog.asksaveasfilename(title=prompt, defaultextension=".csv",
                                         initialfile=default_name, filetypes=[("CSV files","*.csv")])
        if p: return p
    if not QUIET:
        s = input(f"{prompt}ï¼ˆé è¨­ {default_name}ï¼‰ï¼š").strip()
        return s if s else default_name
    # QUIET æ¨¡å¼ï¼šç›´æ¥ç”¨é è¨­æª”åï¼ˆç›¸å°æˆ–çµ•å°çš†å¯ï¼‰
    return default_name

def _detect_encoding(path):
    if not HAS_CHARDET: return "utf-8"
    try:
        with open(path, "rb") as f:
            return chardet.detect(f.read(10000)).get("encoding") or "utf-8"
    except Exception:
        return "utf-8"

def _clean_text(v):
    import re
    return re.sub(r'[^a-z0-9_]', '', str(v).lower()) if v else "unknown"

def _clean_service(v):
    import re
    s = str(v)
    s = re.sub(r'(?:[\s\-_])?port\d+$', "", s, flags=re.IGNORECASE)
    s = re.sub(r'[-_/](\d+)(?:[-_]\d+)?$', "", s)
    s = re.sub(r'[-_]?(to|udp|tcp)[-_]?\d*$', "", s, flags=re.IGNORECASE)
    s = re.sub(r'\s+\d+$', "", s)
    return _clean_text(s)

def parse_log_line(line):
    """K=V è§£æï¼šä¿ç•™ idseqï¼Œä¸è¼¸å‡º raw_logï¼ˆä¸»è¡¨ï¼‰ï¼›è‹¥å•Ÿç”¨å¤–æ›å­—å…¸å†å¦å­˜ rawã€‚"""
    try:
        pairs = KV_PATTERN.findall(line)
        if not pairs: return None
        kv = {k.lower(): v.strip('"\'') for k, v in pairs}
        return {
            "idseq": kv.get("idseq", ""),  # å­—ä¸²å­˜æ”¾ï¼Œé¿å…æ•´æ•¸æº¢ä½
            "date": kv.get("date", ""), "time": kv.get("time", ""), "itime": kv.get("itime", ""),
            "subtype": _clean_text(kv.get("subtype", "unknown")),
            "srcip": kv.get("srcip",""), "srcport": kv.get("srcport","0"),
            "srcintf": _clean_text(kv.get("srcintf","unknown")),
            "dstip": kv.get("dstip",""), "dstport": kv.get("dstport","0"),
            "dstintf": _clean_text(kv.get("dstintf","unknown")),
            "action": _clean_text(kv.get("action","unknown")),
            "sentpkt": kv.get("sentpkt","0"), "rcvdpkt": kv.get("rcvdpkt","0"),
            "duration": kv.get("duration","0"),
            "service": _clean_service(kv.get("service","unknown")),
            "devtype": _clean_text(kv.get("devtype","unknown")),
            "level": _clean_text(kv.get("level","unknown")),
            "crscore": kv.get("crscore","0"),
            "crlevel": _clean_text(kv.get("crlevel","unknown")),
            "raw_line": line.strip()  # åƒ…ä¾›å¤–æ›å­—å…¸ä½¿ç”¨
        }
    except Exception as e:
        logging.error(f"è§£æå¤±æ•—ï¼š{e}")
        return None

def _finalize_datetime(df):
    # å„ªå…ˆ date+timeï¼›å¦å‰‡ itime(epoch ç§’)
    if "date" in df.columns and "time" in df.columns:
        df["datetime"] = pd.to_datetime(df["date"].astype(str) + " " + df["time"].astype(str), errors="coerce")
        df.drop(columns=["date","time"], inplace=True, errors="ignore")
    if "datetime" not in df.columns or not pd.api.types.is_datetime64_any_dtype(df["datetime"]):
        if "itime" in df.columns:
            df["datetime"] = pd.to_datetime(pd.to_numeric(df["itime"], errors="coerce"), unit="s", errors="coerce")
    df.drop(columns=["itime"], inplace=True, errors="ignore")
    return df

def _set_is_attack(df):
    if "crscore" in df.columns:
        df["is_attack"] = (pd.to_numeric(df["crscore"], errors="coerce").fillna(0).astype(int) > 0).astype(int)
    elif "crlevel" in df.columns:
        safe = {"0","unknown","none",""}
        df["is_attack"] = (~df["crlevel"].astype(str).str.lower().isin(safe)).astype(int)
    else:
        df["is_attack"] = 0
    return df

def _reorder_keep_only(df):
    for col in COLUMN_ORDER:
        if col not in df.columns:
            df[col] = "" if col != "datetime" else pd.NaT
    return df[COLUMN_ORDER]

def _choose_mode_interactive():
    print(f"{Fore.CYAN}è«‹é¸æ“‡æ“ä½œæ¨¡å¼ï¼š")
    print("  1. é å…ˆè¨­å®šæŠ½æ¨£ï¼ˆè™•ç†æ™‚åŒæ­¥è¼¸å‡ºæœªæŠ½æ¨£+æŠ½æ¨£ï¼‰")
    print("  2. å¾Œç½®æŠ½æ¨£ï¼ˆå…ˆæ¸…æ´—ï¼Œå†å°æ¸…æ´—æª”äºŒæ¬¡æŠ½æ¨£ï¼‰")
    print("  3. åƒ…æ¸…æ´—ï¼ˆä¸æŠ½æ¨£ï¼‰")
    ans = input("è¼¸å…¥ 1/2/3ï¼ˆé è¨­ 1ï¼‰ï¼š").strip() or "1"
    return "1" if ans not in ("1","2","3") else ans

def _choose_sampling_interactive():
    print(f"{Fore.CYAN}æŠ½æ¨£æ–¹æ³•ï¼š")
    print("  1. éš¨æ©Ÿ random")
    print("  2. å¹³è¡¡ balancedï¼ˆä¾æ¨™ç±¤ä¸‹æ¡æ¨£ï¼‰")
    print("  3. ç³»çµ± systematicï¼ˆå›ºå®šé–“éš”ï¼‰")
    print("  4. è‡ªè¨‚ customï¼ˆlabel:æ•¸é‡, ...ï¼‰")
    m = input("è¼¸å…¥ 1/2/3/4ï¼ˆé è¨­ 1ï¼‰ï¼š").strip() or "1"
    if m not in ("1","2","3","4"): m = "1"
    basis = input("æŠ½æ¨£ä¾æ“šï¼ˆ1=is_attack, 2=crlevelï¼›é è¨­ 1ï¼‰ï¼š").strip() or "1"
    label_col = "crlevel" if basis == "2" else "is_attack"
    seed = input(f"éš¨æ©Ÿç¨®å­ï¼ˆé è¨­ {DEFAULT_SAMPLING_SEED}ï¼‰ï¼š").strip()
    try: seed = int(seed) if seed else DEFAULT_SAMPLING_SEED
    except: seed = DEFAULT_SAMPLING_SEED
    cfg = {"method": m, "label_col": label_col, "seed": seed}
    if m == "1":
        r = input(f"éš¨æ©Ÿæ¯”ä¾‹ 0~1ï¼ˆé è¨­ {DEFAULT_RANDOM_RATIO}ï¼‰ï¼š").strip()
        try: cfg["ratio"] = float(r) if r else DEFAULT_RANDOM_RATIO
        except: cfg["ratio"] = DEFAULT_RANDOM_RATIO
    elif m == "4":
        print("æ ¼å¼ç¤ºä¾‹ï¼š0:1000,1:1000,2:200")
        cc = input("custom_countsï¼š").strip()
        d = {}
        if cc:
            for part in cc.split(","):
                try:
                    k,v = part.split(":")
                    d[k.strip()] = int(v.strip())
                except: pass
        cfg["custom_counts"] = d
    return cfg

def _detect_encoding_safe(path):
    enc = _detect_encoding(path)
    return enc

def _write_rawdict_open():
    if not DEFAULT_WRITE_RAWDICT: return None
    return gzip.open(RAWDICT_GZ_PATH, "at", encoding="utf-8")

def _write_rawdict_line(gzfp, rec):
    if gzfp is None: return
    rid = rec.get("idseq","")
    raw = rec.get("raw_line","")
    if rid and raw:
        gzfp.write(json.dumps({"idseq": rid, "raw": raw}, ensure_ascii=False) + "\n")

# -------------------- ä¸»ç¨‹åºï¼ˆå¯éœé»˜ï¼‰ --------------------
def clean_logs(
    quiet: bool = None,
    mode: str = None,
    paths: list = None,
    clean_csv: str = "processed_logs.csv",
    sampled_csv: str = None,
    sampling_cfg: dict = None
):
    """
    æ¸…æ´—ä¸»å‡½å¼ï¼ˆä¾› pipeline/UI å‘¼å«ï¼‰ï¼š
      - quiet=Trueï¼šå®Œå…¨éœé»˜ï¼Œä¸é€²è¡Œä»»ä½•äº’å‹•å°å‡ºï¼›éœ€æä¾› pathsï¼›å…¶å®ƒåƒæ•¸å¯çœç•¥ç”¨é è¨­
      - quiet=False æˆ– Noneï¼šå¦‚æœªæä¾›åƒæ•¸ï¼Œé€²å…¥äº’å‹•å¼å•ç­”ï¼ˆGUI/CLIï¼‰
    å›å‚³ï¼šclean_csv çš„å¯¦éš›è¼¸å‡ºè·¯å¾‘
    """
    global QUIET
    if quiet is not None:
        QUIET = bool(quiet)

    if not QUIET:
        print(f"{Fore.WHITE}{Style.BRIGHT}==== æ¸…æ´— / æ¨™æº–åŒ–ï¼ˆæµå¼ï¼‰ ====")

    # äº’å‹•æˆ–éœé»˜æ¨¡å¼ä¸‹çš„åƒæ•¸æº–å‚™
    if mode is None:
        mode = _choose_mode_interactive() if not QUIET else "3"
    if paths is None:
        paths = _select_files_interactive() if not QUIET else None
    if QUIET and not paths:
        raise ValueError("QUIET æ¨¡å¼éœ€æä¾› pathsï¼ˆlist[str]ï¼‰ã€‚")

    # è¼¸å‡ºæª”åï¼ˆéœé»˜å‰‡ä½¿ç”¨é è¨­ï¼‰
    if sampled_csv is None and mode in ("1","2"):  # éœ€è¦æŠ½æ¨£è¼¸å‡º
        sampled_csv = "sampled_logs.csv"

    if not QUIET:
        clean_csv = _select_save_path_interactive("é¸æ“‡æ¸…æ´—å¾Œï¼ˆæœªæŠ½æ¨£ï¼‰CSV å„²å­˜ä½ç½®", clean_csv)
        if mode in ("1","2"):
            sampled_csv = _select_save_path_interactive("é¸æ“‡æŠ½æ¨£å¾Œ CSV å„²å­˜ä½ç½®", sampled_csv or "sampled_logs.csv")

    # æŠ½æ¨£è¨­å®š
    if sampling_cfg is None:
        if mode in ("1","2"):
            sampling_cfg = _choose_sampling_interactive() if not QUIET else {
                "method": "1", "ratio": DEFAULT_RANDOM_RATIO,
                "label_col": "is_attack", "seed": DEFAULT_SAMPLING_SEED
            }
        else:
            sampling_cfg = {"method":"1","ratio":1.0,"label_col":"is_attack","seed":DEFAULT_SAMPLING_SEED}

    method_map = {"1":"random","2":"balanced","3":"systematic","4":"custom"}
    method = method_map.get(sampling_cfg.get("method","1"), "random")
    ratio = float(sampling_cfg.get("ratio", DEFAULT_RANDOM_RATIO))
    label_col = sampling_cfg.get("label_col","is_attack")
    seed = int(sampling_cfg.get("seed", DEFAULT_SAMPLING_SEED))
    custom_counts = sampling_cfg.get("custom_counts", None)
    np.random.seed(seed)

    uniques = {c: set() for c in UNIQUE_COLS}
    first_clean, first_sample = True, True
    tot_clean, tot_sample = 0, 0
    rawdict_fp = _write_rawdict_open()

    def _process_df(df):
        nonlocal first_clean, first_sample, tot_clean, tot_sample
        # å®Œæˆæ™‚é–“ã€æ¨™ç±¤ã€å»é‡ã€é‡æ’
        df = _finalize_datetime(df)
        df = _set_is_attack(df)
        df.drop_duplicates(inplace=True)
        df = _reorder_keep_only(df)
        # [1] å¯«æ¸…æ´—æª”
        df.to_csv(clean_csv, mode="w" if first_clean else "a",
                  header=first_clean, index=False, encoding="utf-8")
        first_clean = False
        tot_clean += len(df)
        # [2] è¨˜æ†¶é«”æª¢æŸ¥èˆ‡ flush
        check_and_flush("log_cleaning", df)
        # [3] å¯«æŠ½æ¨£æª”ï¼ˆè¦–æ¨¡å¼ï¼‰
        if sampled_csv:
            if method == "random":
                sdf = df.sample(frac=min(max(ratio,0.0),1.0), random_state=seed) if ratio < 1.0 else df
            elif method == "balanced":
                if label_col not in df.columns:
                    if not QUIET:
                        print(f"{Fore.RED}âŒ æ‰¾ä¸åˆ°å¹³è¡¡æ¬„ä½ {label_col}ï¼Œæœ¬å¡Šè·³éæŠ½æ¨£")
                    sdf = df.head(0)
                else:
                    vc = df[label_col].value_counts()
                    m = vc.min() if len(vc)>0 else 0
                    parts = []
                    for _, g in df.groupby(label_col):
                        n = min(m, len(g))
                        if n>0:
                            parts.append(g.sample(n=n, random_state=seed, replace=(n>len(g))))
                    sdf = pd.concat(parts).sample(frac=1.0, random_state=seed) if parts else df.head(0)
            elif method == "systematic":
                sdf = df.iloc[::max(int(1.0/ratio),1)] if ratio < 1.0 and ratio>0 else df
            else:
                if (custom_counts is None) or (label_col not in df.columns):
                    if not QUIET:
                        print(f"{Fore.RED}âŒ custom æœªæ­£ç¢ºè¨­å®šï¼Œè·³éæŠ½æ¨£")
                    sdf = df.head(0)
                else:
                    parts = []
                    for k, n in custom_counts.items():
                        g = df[df[label_col].astype(str) == str(k)]
                        if len(g)==0: continue
                        take = min(int(n), len(g))
                        parts.append(g.sample(n=take, random_state=seed, replace=(take>len(g))))
                    sdf = pd.concat(parts).sample(frac=1.0, random_state=seed) if parts else df.head(0)
            sdf.to_csv(sampled_csv, mode="w" if first_sample else "a",
                       header=first_sample, index=False, encoding="utf-8")
            first_sample = False
            tot_sample += len(sdf)

    for path in paths:
        opener = gzip.open if path.endswith(".gz") else open
        enc = _detect_encoding_safe(path)
        try:
            with opener(path, "rt", encoding=enc, errors="replace") as f:
                buf = []
                if not QUIET:
                    print(f"{Fore.MAGENTA}ğŸ“ è™•ç†æª”æ¡ˆï¼š{os.path.basename(path)}")
                for i, line in enumerate(tqdm(f, desc=os.path.basename(path) if not QUIET else None, unit="è¡Œ", disable=QUIET)):
                    rec = parse_log_line(line)
                    if rec:
                        # å¯«å¤–æ›å­—å…¸ï¼ˆå¯é¸ï¼‰
                        _write_rawdict_line(rawdict_fp, rec)
                        # æ”¶é›†å”¯ä¸€å€¼
                        for k in UNIQUE_COLS:
                            uniques[k].add(rec.get(k,"") or "unknown")
                        buf.append(rec)
                    if len(buf) >= CHUNK_LINES:
                        _process_df(pd.DataFrame(buf))
                        buf = []
                if buf:
                    _process_df(pd.DataFrame(buf))
        except Exception as e:
            logging.error(f"è®€å–å¤±æ•—ï¼š{path} - {e}")
            if not QUIET:
                print(f"{Fore.RED}æª”æ¡ˆè®€å–éŒ¯èª¤ï¼š{path}")

    # å”¯ä¸€å€¼æ¸…å–®
    uniq = {k: sorted(list(v)) for k,v in uniques.items()}
    with open("log_unique_values.json", "w", encoding="utf-8") as fj:
        json.dump(uniq, fj, ensure_ascii=False, indent=2)
    with open("log_unique_values.txt", "w", encoding="utf-8") as ft:
        for k in uniq:
            ft.write(f"{k}: {', '.join(uniq[k])}\n")

    if rawdict_fp is not None:
        rawdict_fp.close()
        if not QUIET:
            print(f"{Fore.GREEN}âœ… å·²è¼¸å‡º idseqâ†’raw_log å­—å…¸ï¼š{RAWDICT_GZ_PATH}")

    if not QUIET:
        print(f"{Fore.GREEN}âœ… æ¸…æ´—å®Œæˆï¼š{clean_csv}ï¼ˆ{tot_clean}ï¼‰")
        if sampled_csv:
            print(f"{Fore.GREEN}âœ… æŠ½æ¨£å®Œæˆï¼š{sampled_csv}ï¼ˆ{tot_sample}ï¼‰")
    return clean_csv

def main():
    # åƒ…ç•¶ä½¿ç”¨è€…ç›´æ¥åŸ·è¡Œæœ¬æª”ï¼Œæ‰é€²å…¥äº’å‹•å¼
    clean_logs(quiet=False)

if __name__ == "__main__":
    main()
