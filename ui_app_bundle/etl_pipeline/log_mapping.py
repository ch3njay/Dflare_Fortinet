# -*- coding: utf-8 -*-
"""
log_mapping.py
è·è²¬ï¼š
- è®€å–æ¸…æ´—å¾Œçš„ CSVï¼ˆå« idseqï¼‰ï¼Œåªåšã€å­—å…¸æ˜ å°„ã€‘èˆ‡ã€æ¬„ä½æ’åºã€‘
- idseq ç½®ç¬¬ä¸€æ¬„ï¼›raw_log è‹¥å­˜åœ¨å‰‡ç§»é™¤
- æª¢æŸ¥å”¯ä¸€å€¼æ¸…å–®æ˜¯å¦è¦†è“‹ï¼ˆå¯é¸ï¼‰ï¼›æœªè¦†è“‹è€…è¨˜éŒ„å ±å‘Š
- æµå¼åˆ†å¡Šï¼ˆTB ç­‰ç´šï¼‰ã€tqdm é€²åº¦æ¢ã€colorama è‰²å½©ã€CLI é˜²ç¬¨
- QUIET æ——æ¨™å¯é—œé–‰æ‰€æœ‰æç¤ºåˆ—å°ï¼ˆä¾› pipeline/UI éœé»˜å‘¼å«ï¼‰
"""

import os, json
import pandas as pd
from tqdm import tqdm
from colorama import Fore, Style, init as colorama_init
from .utils import check_and_flush

# ---- åˆå§‹åŒ– ----
colorama_init(autoreset=True)

# å¯éœé»˜çš„å…¨åŸŸæ——æ¨™
QUIET = False

# ---- CONFIG ----
CSV_CHUNK_SIZE = 100_000
DEFAULT_INPUT  = "processed_logs.csv"
DEFAULT_OUTPUT = "preprocessed_data.csv"
DEFAULT_UNIQUE = "log_unique_values.json"  # ç”± cleaning ç”¢å‡ºçš„å”¯ä¸€å€¼æ¸…å–®ï¼ˆå¯ç„¡ï¼‰

# â€”â€” æ ¸å¿ƒæ¬„ä½é †åºï¼ˆidseq ç½®é ‚ï¼›ä¸å« raw_logï¼‰â€”â€”
CORE_ORDER = [
    "idseq", "datetime", "subtype",
    "srcip", "srcport", "srcintf",
    "dstip", "dstport", "dstintf",
    "action", "sentpkt", "rcvdpkt",
    "duration", "service", "devtype", "level",
    "crscore", "crlevel", "is_attack"
]

# â€”â€” é¡åˆ¥æ˜ å°„å­—å…¸ï¼ˆä¾éœ€è¦æ“´å……ï¼›æœªçŸ¥å€¼ â†’ -1ï¼‰â€”â€”
CATEGORICAL_MAPPINGS = {
    "subtype": {
        "unknown": 0, "forward": 1, "local": 2, "app-ctrl": 3, "appctrl": 3,
        "webfilter": 4, "ssl": 5, "anomaly": 6, "ips": 7, "dos": 8, "voip": 9,
        "virus": 10, "multicast": 11, "email": 12, "application": 19, "traffic": 24,
        "system": 22, "user": 23, "vpn": 25, "ddos": 26, "portscan": 27
    },
    "srcintf": {"unknown": 0, "root": 1, "port4": 2, "port7": 3, "port9": 4, "vlan19": 6, "vlan250": 7},
    "dstintf": {"unknown": 0, "root": 1, "port1": 2, "port2": 3, "port3": 4, "port4": 5, "vlan1": 12, "vlan19": 13},
    "action":  {"unknown": 0, "accept": 1, "deny": 2, "block": 3, "close": 7, "detect": 8, "timeout": 9, "drop": 15},
    "devtype": {"unknown": -1, "router": 1},
    "crlevel": {"unknown": 0,  "low": 1, "medium": 2, "high": 3, "critical": 4},
    "level":   {"unknown": 0, "notice": 1, "warning": 2, "error": 3, "information": 4, "critical": 5}
}

UNIQUE_CHECK_COLS = ["subtype","level","srcintf","dstintf","action","service","devtype","crlevel"]

# ---- å·¥å…· ----
# ---- å·¥å…· ----
def _ask_path(prompt, default_path):
    if QUIET:
        return default_path
    p = input(Fore.CYAN + f"{prompt}ï¼ˆé è¨­ {default_path}ï¼‰ï¼š").strip()
    return p if p else default_path

def _reorder_preserve(df: pd.DataFrame) -> pd.DataFrame:
    for c in CORE_ORDER:
        if c not in df.columns:
            df[c] = "" if c != "datetime" else pd.NaT
    rest = [c for c in df.columns if c not in CORE_ORDER and c != "raw_log"]
    return df[CORE_ORDER + rest]

def _normalize_str(s: pd.Series) -> pd.Series:
    return s.astype(str).str.strip().str.lower().fillna("unknown")

def _build_dynamic_mapping(col_values, base: dict = None):
    """
    ä¾å”¯ä¸€å€¼æ¸…å–®å»ºç«‹ç©©å®šæ˜ å°„è¡¨ï¼ˆunknownâ†’0ï¼›å…¶é¤˜ä¾å­—æ¯åºï¼è‡ªç„¶åºï¼‰ã€‚
    è‹¥æä¾› baseï¼Œæœƒå„ªå…ˆå¥—ç”¨ base ä¸­å·²å®šç¾©çš„å€¼ï¼ˆä¿ç•™ä½ çš„æ‰‹å‹•è¡¨ï¼‰ã€‚
    """
    base = base.copy() if base else {}
    mapping = {}
    # å…ˆæ”¾ unknown
    mapping["unknown"] = base.get("unknown", 0)
    used = set(mapping.values())

    # å…¶é¤˜ä¾åºç·¨ç¢¼ï¼ˆè·³éå·²å­˜åœ¨æ–¼ base çš„ï¼‰
    pool = sorted({str(v).lower() for v in col_values if str(v).strip()})
    next_id = max(used) + 1 if used else 1
    for v in pool:
        if v in mapping: 
            continue
        if v in base:
            mapping[v] = base[v]
            used.add(base[v])
        else:
            while next_id in used:
                next_id += 1
            mapping[v] = next_id
            used.add(next_id)
            next_id += 1
    return mapping

def _load_unique_values(path: str):
    if not os.path.exists(path):
        if not QUIET:
            print(Fore.YELLOW + f"âš ï¸ æ‰¾ä¸åˆ°å”¯ä¸€å€¼æ¸…å–®ï¼š{path}ï¼Œè·³éè¦†è“‹æª¢æŸ¥")
        return {}, False
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f), True
    except Exception:
        if not QUIET:
            print(Fore.YELLOW + f"âš ï¸ ç„¡æ³•è®€å–å”¯ä¸€å€¼æ¸…å–®ï¼š{path}ï¼Œè·³éè¦†è“‹æª¢æŸ¥")
        return {}, False

def _check_coverage(chunk: pd.DataFrame, uniq_map: dict, missing: dict):
    for col in UNIQUE_CHECK_COLS:
        if col in chunk.columns and col in uniq_map:
            seen = set(map(str, chunk[col].astype(str).unique()))
            valid = set(map(str, uniq_map[col]))
            diff = seen - valid
            if diff:
                missing.setdefault(col, set()).update(diff)

def _apply_mappings(df: pd.DataFrame, uniq_map: dict = None) -> pd.DataFrame:
    """
    å¼·åŒ–ç‰ˆï¼š
    - æ—¢æœ‰æ‰‹å‹•æ˜ å°„ï¼šsubtype/srcintf/dstintf/action/devtype/crlevel/levelï¼ˆç¶­æŒï¼‰
    - æ–°å¢ï¼šservice äº¦åšæ˜ å°„ï¼ˆä½¿ç”¨å”¯ä¸€å€¼æ¸…å–®å‹•æ…‹å»ºè¡¨ï¼Œunknownâ†’0ï¼‰
    """
    # 1) å›ºå®šæ¬„ä½ï¼šä½¿ç”¨é å…ˆå®šç¾©çš„ CATEGORICAL_MAPPINGS
    for col, mapping in CATEGORICAL_MAPPINGS.items():
        if col not in df.columns:
            continue
        if pd.api.types.is_numeric_dtype(df[col]):
            continue
        s = _normalize_str(df[col])
        df[col] = s.map(lambda x: mapping.get(x, -1)).astype("int32", errors="ignore")

    # 2) å‹•æ…‹ service æ˜ å°„ï¼ˆè‹¥å­˜åœ¨ï¼‰
    if "service" in df.columns:
        if pd.api.types.is_numeric_dtype(df["service"]):
            # å·²æ˜¯æ•¸å€¼å°±ä¸è™•ç†
            pass
        else:
            s = _normalize_str(df["service"])
            # å„ªå…ˆä½¿ç”¨å”¯ä¸€å€¼æ¸…å–®ï¼›å¦å‰‡ä»¥ç•¶å‰ chunk ä¼°è¨ˆï¼ˆç©©å®šæ€§è¼ƒå·®ï¼Œä½†èƒ½ä¿å·¥ä½œæµç¨‹ä¸ä¸­æ–·ï¼‰
            if uniq_map and "service" in uniq_map and uniq_map["service"]:
                dyn_map = _build_dynamic_mapping(uniq_map["service"])
            else:
                dyn_map = _build_dynamic_mapping(s.unique())
            df["service"] = s.map(lambda x: dyn_map.get(x, 0)).astype("int32", errors="ignore")

    return df

# ---- ä¸»ç¨‹åºï¼ˆä¿ç•™ CLIï¼›å¯éœé»˜ï¼‰ ----
def main():
    if not QUIET:
        print(Style.BRIGHT + "==== æ˜ å°„ / æ’åºï¼ˆlog_mappingï¼‰====")
    in_csv  = _ask_path("è¼¸å…¥æª”ï¼ˆprocessed_logs.csvï¼‰", DEFAULT_INPUT)
    out_csv = _ask_path("è¼¸å‡ºæª”ï¼ˆpreprocessed_data.csvï¼‰", DEFAULT_OUTPUT)
    uniq_p  = _ask_path("å”¯ä¸€å€¼æ¸…å–®ï¼ˆå¯æŒ‰ Enter ç•¥éï¼‰", DEFAULT_UNIQUE)

    if not os.path.exists(in_csv):
        if not QUIET:
            print(Fore.RED + f"âŒ æ‰¾ä¸åˆ°è¼¸å…¥æª”ï¼š{in_csv}")
        return

    uniq_map, do_check = _load_unique_values(uniq_p)
    first = True
    total = 0
    missing = {}

    for chunk in tqdm(pd.read_csv(in_csv, chunksize=CSV_CHUNK_SIZE, encoding="utf-8"),
                    desc=("åˆ†å¡Šè™•ç†" if not QUIET else None), unit="chunk", disable=QUIET):
        if "raw_log" in chunk.columns:
            chunk.drop(columns=["raw_log"], inplace=True)
        if "datetime" in chunk.columns and not pd.api.types.is_datetime64_any_dtype(chunk["datetime"]):
            chunk["datetime"] = pd.to_datetime(chunk["datetime"], errors="coerce")

        # è¦†è“‹æª¢æŸ¥åœ¨æ˜ å°„å‰
        if do_check:
            _check_coverage(chunk, uniq_map, missing)

        # å‚³å…¥ uniq_map ç¢ºä¿ service ç©©å®šæ˜ å°„
        chunk = _apply_mappings(chunk, uniq_map)
        # 
        check_and_flush("log_mapping", chunk)
        
        if "is_attack" not in chunk.columns:
            if "crscore" in chunk.columns:
                chunk["is_attack"] = (pd.to_numeric(chunk["crscore"], errors="coerce").fillna(0).astype(int) > 0).astype(int)
            else:
                chunk["is_attack"] = 0

        chunk.drop_duplicates(inplace=True)
        chunk = _reorder_preserve(chunk)

        chunk.to_csv(out_csv, mode="w" if first else "a", header=first, index=False, encoding="utf-8")
        first = False
        total += len(chunk)
        if not QUIET:
            print(Fore.GREEN + f"è™•ç† {len(chunk)} ç­†ï¼Œç¸½è¨ˆ {total} ç­†")

    # å ±å‘Š
    report_path = os.path.splitext(out_csv)[0] + "_mapping_report.json"
    rep = {"total_rows": total}
    rep["uncovered_values"] = {k: sorted(list(v)) for k, v in missing.items()} if missing else "none or not-checked"
    with open(report_path, "w", encoding="utf-8") as f:
        json.dump(rep, f, ensure_ascii=False, indent=2)

    if not QUIET:
        print(Fore.GREEN + f"âœ… å®Œæˆï¼š{out_csv}ï¼ˆ{total} ç­†ï¼‰")
        print(Fore.GREEN + f"ğŸ“ å ±å‘Šï¼š{report_path}")

if __name__ == "__main__":
    main()
