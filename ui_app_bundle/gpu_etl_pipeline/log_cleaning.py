# -*- coding: utf-8 -*-
"""
log_cleaning.pyï¼ˆGPU ç‰ˆ, æ”¹è‰¯ï¼šè¼¸å‡º manifest èˆ‡ active_clean_fileï¼‰
- åˆ†å¡Šæµå¼æ¸…æ´—ï¼ˆTB ç­‰ç´šã€é˜² OOMï¼‰
- é€²åº¦æ¢ï¼ˆtqdmï¼‰ã€è‰²å½©è¼¸å‡ºï¼ˆcoloramaï¼‰
- QUIET éœé»˜æ¨¡å¼ï¼šä¾› pipeline/UI å‘¼å«ï¼›é QUIET æ™‚æ”¯æ´ tkinter/CLI äº’å‹•
- ä¸»è³‡æ–™ï¼šç§»é™¤ raw_logï¼›ä¿ç•™ idseq ä¸¦ç½®ç¬¬ä¸€æ¬„
- å³æ™‚é›™æª”è¼¸å‡ºï¼ˆæœªæŠ½æ¨£ + æŠ½æ¨£ï¼‰
- å”¯ä¸€å€¼æ¸…å–®ï¼ˆjson/txtï¼‰
- âœ… æ–°å¢ï¼šç”¢å‡º/æ›´æ–° manifest.jsonï¼Œä¸¦å¯«å…¥ active_clean_file
"""
import os, re, gzip, json, time, logging
from collections import defaultdict
from tqdm import tqdm
from colorama import init as colorama_init, Fore, Style

# ---- åŒ¯å…¥ç›¸å®¹å±¤ ----
try:
    from .utils import check_and_flush, _HAS_CUDF, _HAS_CUPY
except Exception:
    # å…è¨±å–®æª”åŸ·è¡Œ
    from utils import check_and_flush, _HAS_CUDF, _HAS_CUPY

# ---- è³‡æ–™æ¡†ç›¸å®¹å±¤ï¼ˆå„ªå…ˆ cudfï¼Œå¦å‰‡ pandasï¼‰ ----
if _HAS_CUDF:
    import cudf as xdf
else:
    import pandas as xdf  # type: ignore

# å·¥å…·åº«ï¼šcupy/numpy äºŒé¸ä¸€
try:
    import cupy as xp  # type: ignore
    _XP = "cupy"
except Exception:
    import numpy as xp  # type: ignore
    _XP = "numpy"

# å¯é¸ï¼šGUI
try:
    import tkinter as tk
    from tkinter import filedialog
    TK_OK = True
except Exception:
    TK_OK = False

# å¯é¸ï¼šchardet
try:
    import chardet
    HAS_CHARDET = True
except Exception:
    HAS_CHARDET = False

colorama_init(autoreset=True)
logging.basicConfig(filename="gpu_log_cleaning_error.log", level=logging.ERROR,
                    format="%(asctime)s - %(levelname)s - %(message)s")

# =====================[ CONFIG ]=====================
CHUNK_LINES = 50_000
DEFAULT_SAMPLING_SEED = 42
DEFAULT_RANDOM_RATIO = 1.0
DEFAULT_WRITE_RAWDICT = False
RAWDICT_GZ_PATH = "rawlog_dict.jsonl.gz"

# æ¬„ä½é †åºï¼ˆæ ¸å¿ƒè¼¸å‡ºï¼›ç¬¬ä¸€æ¬„ idseqï¼›ç„¡ raw_logï¼‰
COLUMN_ORDER = [
    'idseq', 'datetime', 'subtype',
    'srcip','srcport','srcintf',
    'dstip','dstport','dstintf',
    'action','sentpkt','rcvdpkt',
    'duration','service','devtype','level',
    'crscore','crlevel','is_attack'
]

UNIQUE_COLS = ["subtype", "level", "srcintf", "dstintf", "action", "service", "devtype", "crlevel"]
# ====================================================

KV_PATTERN = re.compile(r'(\w+)=(".*?"|\'.*?\'|[^"\',\s]+)')
QUIET = False


# --- helper for tests ---
def _split_sampling_targets(df):
    """Return rows that are attacks with known crlevel and the remaining rows."""
    targets_idx = [
        i for i, (c, a) in enumerate(zip(df["crlevel"], df["is_attack"]))
        if a == 1 and c not in {"unknown", "none"}
    ]
    others_idx = [i for i in range(len(df["crlevel"])) if i not in targets_idx]
    return df.select(targets_idx), df.select(others_idx)

# -------------------- å·¥å…· --------------------
def _get_tk_root():
    root = tk.Tk(); root.withdraw(); return root

def _select_files_interactive():
    if (not QUIET) and TK_OK:
        print(f"{Fore.WHITE}ã€æç¤ºã€‘ğŸ“‚ é¡¯ç¤ºæª”æ¡ˆé¸æ“‡ï¼ˆå¯å¤šé¸ï¼‰")
        _get_tk_root()
        paths = filedialog.askopenfilenames(
            title="é¸æ“‡æ—¥èªŒæª”æ¡ˆ",
            filetypes=[("Log/CSV files", "*.txt *.csv *.gz"), ("All files","*.*")]
        )
        if paths: return list(paths)
    while not QUIET:
        s = input("è«‹è¼¸å…¥æ—¥èªŒæª”è·¯å¾‘ï¼ˆå¯å¤šå€‹ï¼Œåˆ†è™Ÿ;åˆ†éš”ï¼‰ï¼š").strip()
        if s:
            paths = [p.strip() for p in s.split(";") if p.strip()]
            if all(os.path.exists(p) for p in paths):
                return paths
        print("âŒ è·¯å¾‘éŒ¯èª¤ï¼Œè«‹é‡è©¦ã€‚")
    raise ValueError("QUIET æ¨¡å¼éœ€ç”±å‘¼å«ç«¯æä¾› paths åƒæ•¸ã€‚")

def _select_save_path_interactive(prompt, default_name):
    if (not QUIET) and TK_OK:
        _get_tk_root()
        p = filedialog.asksaveasfilename(title=prompt, defaultextension=".csv",
                                         initialfile=default_name, filetypes=[("CSV files","*.csv")])
        if p: return p
    if not QUIET:
        s = input(f"{prompt}ï¼ˆé è¨­ {default_name}ï¼‰ï¼š").strip()
        return s if s else default_name
    return default_name

def _detect_encoding(path):
    if not HAS_CHARDET: return "utf-8"
    try:
        with open(path, "rb") as f:
            return chardet.detect(f.read(10000)).get("encoding") or "utf-8"
    except Exception:
        return "utf-8"

def _clean_text(v):
    import re
    return re.sub(r'[^a-z0-9_]', '', str(v).lower()) if v else "unknown"

def _clean_service(v):
    import re
    s = str(v)
    s = re.sub(r'(?:[\s\-_])?port\d+$', "", s, flags=re.IGNORECASE)
    s = re.sub(r'[-_/](\d+)(?:[-_]\d+)?$', "", s)
    s = re.sub(r'[-_]?(to|udp|tcp)[-_]?\d*$', "", s, flags=re.IGNORECASE)
    s = re.sub(r'\s+\d+$', "", s)
    return _clean_text(s)

def parse_log_line(line):
    """K=V è§£æï¼šä¿ç•™ idseqï¼Œä¸è¼¸å‡º raw_logï¼ˆä¸»è¡¨ï¼‰ï¼›è‹¥å•Ÿç”¨å¤–æ›å­—å…¸å†å¦å­˜ rawã€‚"""
    try:
        pairs = KV_PATTERN.findall(line)
        if not pairs: return None
        kv = {k.lower(): v.strip('"\'') for k, v in pairs}
        return {
            "idseq": kv.get("idseq", ""),
            "date": kv.get("date", ""), "time": kv.get("time", ""), "itime": kv.get("itime", ""),
            "subtype": _clean_text(kv.get("subtype", "unknown")),
            "srcip": kv.get("srcip",""), "srcport": kv.get("srcport","0"),
            "srcintf": _clean_text(kv.get("srcintf","unknown")),
            "dstip": kv.get("dstip",""), "dstport": kv.get("dstport","0"),
            "dstintf": _clean_text(kv.get("dstintf","unknown")),
            "action": _clean_text(kv.get("action","unknown")),
            "sentpkt": kv.get("sentpkt","0"), "rcvdpkt": kv.get("rcvdpkt","0"),
            "duration": kv.get("duration","0"),
            "service": _clean_service(kv.get("service","unknown")),
            "devtype": _clean_text(kv.get("devtype","unknown")),
            "level": _clean_text(kv.get("level","unknown")),
            "crscore": kv.get("crscore","0"),
            "crlevel": _clean_text(kv.get("crlevel","unknown")),
            "raw_line": line.strip()
        }
    except Exception as e:
        logging.error(f"è§£æå¤±æ•—ï¼š{e}")
        return None

def _finalize_datetime(df):
    # å„ªå…ˆ date+timeï¼›å¦å‰‡ itime(epoch ç§’)
    try:
        has_datetime = ("date" in df.columns) and ("time" in df.columns)
    except Exception:
        has_datetime = False

    if has_datetime:
        dt = (df["date"].astype("str") + " " + df["time"].astype("str"))
        try:
            df["datetime"] = xdf.to_datetime(dt, errors="coerce")
        except Exception:
            df["datetime"] = xdf.to_datetime(dt)
        for c in ("date","time"):
            try:
                df.drop(columns=[c], inplace=True)
            except Exception:
                pass

    if "datetime" not in df.columns:
        if "itime" in df.columns:
            try:
                df["datetime"] = xdf.to_datetime(df["itime"].astype("int64"), unit="s")
            except Exception:
                df["datetime"] = xdf.to_datetime(df["itime"], unit="s")
    try:
        df.drop(columns=["itime"], inplace=True)
    except Exception:
        pass
    return df

def _set_is_attack(df):
    if "crscore" in df.columns:
        try:
            cs = df["crscore"].astype("int32")
        except Exception:
            cs = df["crscore"].astype("str").astype("int32")
        df["is_attack"] = (cs > 0).astype("int8")
    elif "crlevel" in df.columns:
        safe = {"0","unknown","none",""}
        try:
            mask = ~df["crlevel"].astype("str").str.lower().isin(list(safe))
        except Exception:
            mask = ~df["crlevel"].astype("str").isin(list(safe))
        df["is_attack"] = mask.astype("int8")
    else:
        df["is_attack"] = 0
    return df

def _enforce_crlevel_rule(df):
    """è‹¥ crscore==0ï¼Œcrlevel ä¸€å¾‹æ”¹ç‚º 'none'"""
    if "crscore" in df.columns and "crlevel" in df.columns:
        try:
            cs = df["crscore"].astype("int32")
        except Exception:
            cs = df["crscore"].astype("str").astype("int32")
        mask = (cs == 0)
        if mask.any():
            df.loc[mask, "crlevel"] = "none"
    return df

def _reorder_keep_only(df):
    # ç¢ºä¿æ¬„ä½å­˜åœ¨
    for col in COLUMN_ORDER:
        if col not in df.columns:
            df[col] = xdf.NaT if col == "datetime" else ""
    # åƒ…ä¿ç•™æ ¸å¿ƒæ¬„ä½
    return df[COLUMN_ORDER]

def _choose_mode_interactive():
    print(f"{Fore.CYAN}è«‹é¸æ“‡æ“ä½œæ¨¡å¼ï¼š")
    print("  1. é å…ˆè¨­å®šæŠ½æ¨£ï¼ˆè™•ç†æ™‚åŒæ­¥è¼¸å‡ºæœªæŠ½æ¨£ + æŠ½æ¨£ï¼‰")
    print("  2. å¾Œç½®æŠ½æ¨£ï¼ˆå…ˆæ¸…æ´—ï¼Œå†å°æ¸…æ´—æª”äºŒæ¬¡æŠ½æ¨£ï¼‰")
    print("  3. åƒ…æ¸…æ´—ï¼ˆä¸æŠ½æ¨£ï¼‰")
    ans = input("è¼¸å…¥ 1/2/3ï¼ˆé è¨­ 1ï¼‰ï¼š").strip() or "1"
    return "1" if ans not in ("1","2","3") else ans

def _choose_sampling_interactive():
    print(f"{Fore.CYAN}æŠ½æ¨£æ–¹æ³•ï¼š")
    print("  1. éš¨æ©Ÿ random")
    print("  2. å¹³è¡¡ balancedï¼ˆä¾æ¨™ç±¤ä¸‹æ¡æ¨£ï¼‰")
    print("  3. ç³»çµ± systematicï¼ˆå›ºå®šé–“éš”ï¼‰")
    print("  4. è‡ªè¨‚ customï¼ˆlabel:æ•¸é‡, ...ï¼‰")
    m = input("è¼¸å…¥ 1/2/3/4ï¼ˆé è¨­ 1ï¼‰ï¼š").strip() or "1"
    if m not in ("1","2","3","4"): m = "1"
    basis = input("æŠ½æ¨£ä¾æ“šï¼ˆ1=is_attack, 2=crlevelï¼›é è¨­ 1ï¼‰ï¼š").strip() or "1"
    label_col = "crlevel" if basis == "2" else "is_attack"
    seed = input(f"éš¨æ©Ÿç¨®å­ï¼ˆé è¨­ {DEFAULT_SAMPLING_SEED}ï¼‰ï¼š").strip()
    try: seed = int(seed) if seed else DEFAULT_SAMPLING_SEED
    except: seed = DEFAULT_SAMPLING_SEED
    cfg = {"method": m, "label_col": label_col, "seed": seed}
    if m == "1":
        r = input(f"éš¨æ©Ÿæ¯”ä¾‹ 0~1ï¼ˆé è¨­ {DEFAULT_RANDOM_RATIO}ï¼‰ï¼š").strip()
        try: cfg["ratio"] = float(r) if r else DEFAULT_RANDOM_RATIO
        except: cfg["ratio"] = DEFAULT_RANDOM_RATIO
    elif m == "4":
        print("æ ¼å¼ç¤ºä¾‹ï¼š0:1000,1:1000,2:200")
        cc = input("custom_countsï¼š").strip()
        d = {}
        if cc:
            for part in cc.split(","):
                try:
                    k,v = part.split(":")
                    d[k.strip()] = int(v.strip())
                except: pass
        cfg["custom_counts"] = d
    return cfg

def _write_rawdict_open():
    if not DEFAULT_WRITE_RAWDICT: return None
    return gzip.open(RAWDICT_GZ_PATH, "at", encoding="utf-8")

def _write_rawdict_line(gzfp, rec):
    if gzfp is None: return
    rid = rec.get("idseq","")
    raw = rec.get("raw_line","")
    if rid and raw:
        gzfp.write(json.dumps({"idseq": rid, "raw": raw}, ensure_ascii=False) + "\n")

def _df_sample(df, frac=None, n=None, random_state=None, replace=False):
    return df.sample(frac=frac, n=n, random_state=random_state, replace=replace)

def _ensure_dir(p: str):
    os.makedirs(p, exist_ok=True)

def _atomic_write_json(path: str, payload: dict):
    tmp = path + ".tmp"
    with open(tmp, "w", encoding="utf-8") as f:
        json.dump(payload, f, ensure_ascii=False, indent=2)
    os.replace(tmp, path)

# -------------------- ä¸»ç¨‹åºï¼ˆå¯éœé»˜ï¼‰ --------------------
def clean_logs(
    quiet: bool = None,
    mode: str = None,
    paths: list = None,
    clean_csv: str = "processed_logs.csv",
    sampled_csv: str = None,
    sampling_cfg: dict = None,
    enable_sampling: bool = True,
    # é€™æ¬¡æ–°å¢ï¼šartifacts/run ç›®éŒ„ï¼ˆè‹¥æä¾›å°±è½åœ°åˆ°è©²è™•ï¼‰
    run_dir: str = None
):
    """
    æ¸…æ´—ä¸»å‡½å¼ï¼ˆä¾› pipeline/UI å‘¼å«ï¼‰ï¼š
      - quiet=Trueï¼šå®Œå…¨éœé»˜ï¼Œä¸äº’å‹•ï¼›éœ€æä¾› pathsï¼›å…¶å®ƒåƒæ•¸å¯çœç•¥
      - quiet=False æˆ– Noneï¼šå¦‚æœªæä¾›åƒæ•¸ï¼Œé€²å…¥äº’å‹•å¼å•ç­”ï¼ˆGUI/CLIï¼‰
    âœ… æœƒç”¢å‡º/æ›´æ–° manifest.jsonï¼ŒåŒ…å« active_clean_file
    å›å‚³ï¼šclean_csv çš„å¯¦éš›è¼¸å‡ºè·¯å¾‘
    """
    global QUIET
    if quiet is not None:
        QUIET = bool(quiet)

    if not QUIET:
        print(f"{Fore.WHITE}{Style.BRIGHT}==== æ¸…æ´— / æ¨™æº–åŒ–ï¼ˆGPU æµå¼ï¼‰ ====")

    if mode is None:
        mode = _choose_mode_interactive() if not QUIET else ("1" if enable_sampling else "3")
    if paths is None:
        paths = _select_files_interactive() if not QUIET else None
    if QUIET and not paths:
        raise ValueError("QUIET æ¨¡å¼éœ€æä¾› pathsï¼ˆlist[str]ï¼‰ã€‚")

    # è¼¸å‡ºä½ˆå±€
    run_dir = run_dir or os.path.abspath("./artifacts/" + time.strftime("%Y%m%d_%H%M%S"))
    dir_clean = os.path.join(run_dir, "00_clean")
    _ensure_dir(dir_clean)

    clean_csv = clean_csv if os.path.isabs(clean_csv) else os.path.join(dir_clean, os.path.basename(clean_csv))
    if sampled_csv is None and mode in ("1","2") and enable_sampling:
        sampled_csv = "sampled_logs.csv"
    if sampled_csv:
        sampled_csv = sampled_csv if os.path.isabs(sampled_csv) else os.path.join(dir_clean, os.path.basename(sampled_csv))

    if not QUIET:
        clean_csv = _select_save_path_interactive("é¸æ“‡æ¸…æ´—å¾Œï¼ˆæœªæŠ½æ¨£ï¼‰CSV å„²å­˜ä½ç½®", clean_csv)
        if mode in ("1","2") and enable_sampling:
            sampled_csv = _select_save_path_interactive("é¸æ“‡æŠ½æ¨£å¾Œ CSV å„²å­˜ä½ç½®", sampled_csv or os.path.join(dir_clean,"sampled_logs.csv"))

    if sampling_cfg is None:
        if mode in ("1","2") and enable_sampling:
            sampling_cfg = _choose_sampling_interactive() if not QUIET else {
                "method": "1", "ratio": DEFAULT_RANDOM_RATIO,
                "label_col": "is_attack", "seed": DEFAULT_SAMPLING_SEED
            }
        else:
            sampling_cfg = {"method":"1","ratio":1.0,"label_col":"is_attack","seed":DEFAULT_SAMPLING_SEED}

    method_map = {"1":"random","2":"balanced","3":"systematic","4":"custom"}
    method = method_map.get(sampling_cfg.get("method","1"), "random")
    try:
        ratio = float(sampling_cfg.get("ratio", DEFAULT_RANDOM_RATIO))
    except Exception:
        ratio = DEFAULT_RANDOM_RATIO
    label_col = sampling_cfg.get("label_col","is_attack")
    seed = int(sampling_cfg.get("seed", DEFAULT_SAMPLING_SEED))
    custom_counts = sampling_cfg.get("custom_counts", None)

    # å”¯ä¸€å€¼æ”¶é›†
    uniques = {c: set() for c in UNIQUE_COLS}
    first_clean, first_sample = True, True
    tot_clean, tot_sample = 0, 0
    rawdict_fp = _write_rawdict_open()

    # è§£æè¼¸å…¥ï¼ˆå¯å¤šæª”ï¼‰
    def _iter_lines(paths_list):
        for p in paths_list:
            enc = _detect_encoding(p)
            if p.endswith(".gz"):
                import gzip as _gz
                with _gz.open(p, "rt", encoding=enc, errors="ignore") as f:
                    for line in f:
                        yield line
            else:
                with open(p, "r", encoding=enc, errors="ignore") as f:
                    for line in f:
                        yield line

    def _process_df(buf_records):
        nonlocal first_clean, first_sample, tot_clean, tot_sample
        df = xdf.DataFrame(buf_records)
        df = _finalize_datetime(df)
        df = _set_is_attack(df)
        df = _enforce_crlevel_rule(df)
        try:
            df = df.drop_duplicates()
        except Exception:
            pass
        df = _reorder_keep_only(df)

        # [1] å¯«æ¸…æ´—æª”
        df.to_csv(clean_csv, mode="w" if first_clean else "a",
                  header=first_clean, index=False, encoding="utf-8")
        first_clean = False
        tot_clean += len(df)

        # [2] è¨˜æ†¶é«”ä¿è­·
        check_and_flush("gpu_log_cleaning", df)

        # [3] å¯«æŠ½æ¨£ï¼ˆè¦–æ¨¡å¼/è¨­å®šï¼‰
        if sampled_csv and enable_sampling:
            if method == "random":
                if ratio >= 1.0:
                    sdf = df
                elif ratio <= 0.0:
                    sdf = df.head(0)
                else:
                    sdf = _df_sample(df, frac=ratio, random_state=seed, replace=False)
            elif method == "balanced":
                # ä¾ label ä¸‹æ¡æ¨£ï¼ˆç°¡åŒ–ç‰ˆæœ¬ï¼‰
                sdf_parts = []
                try:
                    for k, g in df.groupby(label_col):
                        frac = min(ratio, 1.0)
                        if len(g) > 0:
                            sdf_parts.append(_df_sample(g, frac=frac, random_state=seed))
                    sdf = xdf.concat(sdf_parts) if len(sdf_parts) else df.head(0)
                except Exception:
                    sdf = df.head(0)
            elif method == "systematic":
                try:
                    sdf = df.iloc[::max(1, int(1/ratio))] if ratio > 0 and ratio < 1 else df
                except Exception:
                    sdf = df.head(0)
            elif method == "custom" and isinstance(custom_counts, dict):
                sdf_parts = []
                try:
                    for k, g in df.groupby(label_col):
                        n = int(custom_counts.get(str(k), 0))
                        if n <= 0:
                            continue
                        if len(g) > n:
                            sdf_parts.append(g.sample(n=n, random_state=seed))
                        else:
                            sdf_parts.append(g)
                    sdf = xdf.concat(sdf_parts) if len(sdf_parts) else df.head(0)
                except Exception:
                    sdf = df.head(0)
            else:
                sdf = df.head(0)

            if len(sdf):
                sdf.to_csv(sampled_csv, mode="w" if first_sample else "a",
                           header=first_sample, index=False, encoding="utf-8")
                first_sample = False
                tot_sample += len(sdf)

        # uniquesï¼ˆå¯æ“´å……å¯«å‡º json/txtï¼‰
        for c in uniques.keys():
            try:
                uniques[c].update(set(map(str, df[c].to_pandas().unique())) if hasattr(df[c], "to_pandas") else set(map(str, df[c].unique())))
            except Exception:
                pass

    # ===== ä¸»è®€å–å›åœˆ =====
    buf, n = [], 0
    for line in tqdm(_iter_lines(paths), desc=("è®€å–ä¸­" if not QUIET else None), unit="line", disable=QUIET):
        rec = parse_log_line(line)
        if rec:
            buf.append(rec); n += 1
            if n >= CHUNK_LINES:
                _process_df(buf)
                buf, n = [], 0
    if buf:
        _process_df(buf)

    if rawdict_fp:
        rawdict_fp.close()

    if not QUIET:
        print(Fore.GREEN + f"âœ… æ¸…æ´—å®Œæˆï¼š{clean_csv}ï¼ˆ{tot_clean} ç­†ï¼‰")
        if sampled_csv and enable_sampling:
            print(Fore.GREEN + f"âœ… æŠ½æ¨£å®Œæˆï¼š{sampled_csv}ï¼ˆ{tot_sample} ç­†ï¼‰")

    # ===== ç”¢å‡º/æ›´æ–° manifest.json =====
    manifest_path = os.path.join(run_dir, "manifest.json")
    try:
        os.makedirs(run_dir, exist_ok=True)
        payload = {}
        if os.path.exists(manifest_path):
            with open(manifest_path, "r", encoding="utf-8") as f:
                payload = json.load(f)

        payload.setdefault("run_id", os.path.basename(run_dir))
        payload["root_dir"] = run_dir
        payload["clean"] = payload.get("clean", {})
        payload["clean"]["processed_csv"] = clean_csv
        if sampled_csv and enable_sampling and os.path.exists(sampled_csv):
            payload["clean"]["sampled_csv"] = sampled_csv
            payload["clean"]["mode"] = "preconfigured"
            payload["clean"]["active_clean_file"] = sampled_csv  # âœ… å•Ÿç”¨æŠ½æ¨£ â†’ active æŒ‡å‘æŠ½æ¨£æª”
        else:
            payload["clean"]["sampled_csv"] = payload["clean"].get("sampled_csv", "")
            payload["clean"]["mode"] = payload["clean"].get("mode", "none")
            payload["clean"]["active_clean_file"] = clean_csv     # âœ… æœªæŠ½æ¨£ â†’ active æŒ‡å‘æ¸…æ´—æª”

        _atomic_write_json(manifest_path, payload)
        if not QUIET:
            print(Style.BRIGHT + Fore.GREEN + f"ğŸ§­ manifest å·²æ›´æ–°ï¼š{manifest_path}")
    except Exception as e:
        print(Fore.YELLOW + f"âš ï¸ manifest å¯«å…¥å¤±æ•—ï¼š{e}")

    return clean_csv
