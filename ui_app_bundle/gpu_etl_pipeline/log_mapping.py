# -*- coding: utf-8 -*-
"""
log_mapping.pyï¼ˆGPU ç‰ˆï¼Œæ”¹è‰¯ï¼šé è¨­å¾ manifest.clean.active_clean_file è®€å–ï¼‰
- è®€å–æ¸…æ´—å¾Œçš„ CSVï¼ˆå„ªå…ˆ active_clean_fileï¼‰ï¼Œé€²è¡Œå­—å…¸æ˜ å°„èˆ‡æ¬„ä½æ’åº
- åˆ†å¡Šè®€å–/append è¼¸å‡ºï¼›ä¿ç•™äº’å‹•/éœé»˜
- ç›¸å®¹åŒ¯å…¥å±¤ï¼šå¯åœ¨ package æˆ–å–®æª”åŸ·è¡Œ
"""
import os
import json
import sys
from tqdm import tqdm
from colorama import Fore, Style, init as colorama_init

# === åŒ¯å…¥ç›¸å®¹å±¤ ===
try:
    from .utils import check_and_flush, _HAS_CUDF
except Exception:
    cur_dir = os.path.dirname(os.path.abspath(__file__))
    if cur_dir not in sys.path:
        sys.path.append(cur_dir)
    from utils import check_and_flush, _HAS_CUDF

import pandas as pd
if _HAS_CUDF:
    import cudf as xdf
else:
    import pandas as xdf  # type: ignore

colorama_init(autoreset=True)
QUIET = False

# ---- CONFIG ----
CSV_CHUNK_SIZE = 100_000
DEFAULT_INPUT  = "processed_logs.csv"
DEFAULT_OUTPUT = "preprocessed_data.csv"
DEFAULT_UNIQUE = "log_unique_values.json"

CORE_ORDER = [
    "idseq", "datetime", "subtype",
    "srcip", "srcport", "srcintf",
    "dstip", "dstport", "dstintf",
    "action", "sentpkt", "rcvdpkt",
    "duration", "service", "devtype", "level",
    "crscore", "crlevel", "is_attack"
]

CATEGORICAL_MAPPINGS = {
    "subtype": {
        "unknown": 0, "forward": 1, "local": 2, "app-ctrl": 3, "appctrl": 3,
        "webfilter": 4, "ssl": 5, "anomaly": 6, "ips": 7, "dos": 8, "voip": 9,
        "virus": 10, "multicast": 11, "email": 12, "application": 19, "traffic": 24,
        "system": 22, "user": 23, "vpn": 25, "ddos": 26, "portscan": 27
    },
    "srcintf": {"unknown": 0, "root": 1, "port4": 2, "port7": 3, "port9": 4, "vlan19": 6, "vlan250": 7},
    "dstintf": {"unknown": 0, "port1": 2, "port2": 3, "port3": 4, "port4": 5, "root": 1, "vlan1": 12, "vlan19": 13},
    "action":  {"unknown": 0, "accept": 1, "deny": 2, "block": 3, "close": 7, "detect": 8, "timeout": 9, "drop": 15},
    "devtype": {"unknown": -1, "router": 1},
    "crlevel": {"unknown": 0, "none": 0,  "low": 1, "medium": 2, "high": 3, "critical": 4},
    "level":   {"unknown": 0, "notice": 1, "warning": 2, "error": 3, "information": 4, "critical": 5}
}

UNIQUE_CHECK_COLS = ["subtype","level","srcintf","dstintf","action","service","devtype","crlevel"]

def _ask_path(prompt, default_path):
    if QUIET:
        return default_path
    p = input(Fore.CYAN + f"{prompt}ï¼ˆé è¨­ {default_path}ï¼‰ï¼š").strip()
    return p if p else default_path

def _reorder_preserve(df):
    for c in CORE_ORDER:
        if c not in df.columns:
            df[c] = xdf.NaT if c == "datetime" else ""
    rest = [c for c in df.columns if c not in CORE_ORDER and c != "raw_log"]
    return df[CORE_ORDER + rest]

def _normalize_str(s):
    try:
        return s.astype(str).str.strip().str.lower().fillna("unknown")
    except Exception:
        return s.astype(str).str.lower()

def _build_dynamic_mapping(col_values, base: dict = None):
    base = base.copy() if base else {}
    mapping = {}
    mapping["unknown"] = base.get("unknown", 0)
    used = set(mapping.values())

    pool = sorted({str(v).lower() for v in col_values if str(v).strip()})
    next_id = max(used) + 1 if used else 1
    for v in pool:
        if v in mapping:
            continue
        if v in base:
            mapping[v] = base[v]; used.add(base[v])
        else:
            while next_id in used:
                next_id += 1
            mapping[v] = next_id
            used.add(next_id)
            next_id += 1
    return mapping

def _load_unique_values(path: str):
    if not os.path.exists(path):
        if not QUIET:
            print(Fore.YELLOW + f"âš ï¸ æ‰¾ä¸åˆ°å”¯ä¸€å€¼æ¸…å–®ï¼š{path}ï¼Œè·³éè¦†è“‹æª¢æŸ¥")
        return {}, False
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f), True
    except Exception:
        if not QUIET:
            print(Fore.YELLOW + f"âš ï¸ ç„¡æ³•è®€å–å”¯ä¸€å€¼æ¸…å–®ï¼š{path}ï¼Œè·³éè¦†è“‹æª¢æŸ¥")
        return {}, False

def _check_coverage(chunk, uniq_map: dict, missing: dict):
    import pandas as _pd
    for col in UNIQUE_CHECK_COLS:
        if col in chunk.columns and col in uniq_map:
            seen = set(map(str, _pd.Series(chunk[col]).astype(str).unique()))
            valid = set(map(str, uniq_map[col]))
            diff = seen - valid
            if diff:
                missing.setdefault(col, set()).update(diff)

def _apply_mappings(df, uniq_map: dict = None):
    # 1) å›ºå®šæ¬„ä½
    for col, mapping in CATEGORICAL_MAPPINGS.items():
        if col not in df.columns:
            continue
        s = _normalize_str(df[col])
        try:
            df[col] = s.map(mapping).fillna(-1).astype("int32")
        except Exception:
            df[col] = s.map(lambda x: mapping.get(x, -1))

    # 2) service å‹•æ…‹æ˜ å°„
    if "service" in df.columns:
        if uniq_map and "service" in uniq_map and uniq_map["service"]:
            dyn_map = _build_dynamic_mapping(uniq_map["service"])
        else:
            try:
                values = df["service"].astype(str).unique().to_pandas().tolist()
            except Exception:
                import pandas as _pd
                values = _pd.Series(df["service"]).astype(str).unique().tolist()
            dyn_map = _build_dynamic_mapping(values)
        s = _normalize_str(df["service"])
        try:
            df["service"] = s.map(dyn_map).fillna(0).astype("int32")
        except Exception:
            df["service"] = s.map(lambda x: dyn_map.get(x, 0))
    return df

def _read_manifest(run_dir: str = None) -> dict:
    """
    å˜—è©¦è®€å– run_dir/manifest.jsonï¼›è‹¥ run_dir æœªæä¾›ï¼Œå˜—è©¦å¾ç•¶å‰ç›®éŒ„å‘ä¸Šå°‹æ‰¾ã€‚
    æ‰¾ä¸åˆ°æ™‚å›å‚³ {}
    """
    candidates = []
    if run_dir:
        candidates.append(os.path.join(run_dir, "manifest.json"))
    # å¸¸è¦‹ç›¸å°ä½ç½®
    candidates += [
        os.path.abspath("./manifest.json"),
        os.path.abspath(os.path.join(os.getcwd(), "manifest.json"))
    ]
    for p in candidates:
        if os.path.exists(p):
            try:
                with open(p, "r", encoding="utf-8") as f:
                    return json.load(f)
            except Exception:
                pass
    return {}

def main(in_csv: str = None,
         out_csv: str = None,
         uniq_json: str = None,
         # å…è¨±å¤šé¤˜åƒæ•¸ï¼ˆç”± pipeline å‚³å…¥ï¼‰ï¼Œä¿æŒç›¸å®¹
         batch_mode: bool = None,
         batch_size: int = None,
         quiet: bool = None,
         run_dir: str = None,
         use_manifest: bool = True,
         **kwargs):
    """
    - é è¨­ use_manifest=Trueï¼šå„ªå…ˆè®€ manifest.clean.active_clean_file ä½œç‚ºè¼¸å…¥
    - è‹¥è®€å–å¤±æ•—æˆ–ä¸å­˜åœ¨ï¼Œå›é€€åˆ° in_csv æˆ– DEFAULT_INPUT
    """
    global QUIET, CSV_CHUNK_SIZE
    if quiet is not None:
        QUIET = bool(quiet)
    if batch_size:
        CSV_CHUNK_SIZE = int(batch_size)

    if not QUIET:
        print(Style.BRIGHT + "==== æ˜ å°„ / æ’åºï¼ˆgpu_log_mappingï¼‰====")

    src_from_manifest = None
    if use_manifest:
        m = _read_manifest(run_dir)
        try:
            src_from_manifest = m.get("clean", {}).get("active_clean_file", None)
        except Exception:
            src_from_manifest = None

    # å„ªå…ˆé †åºï¼šmanifest â†’ in_csv â†’ DEFAULT_INPUT
    in_csv  = src_from_manifest or in_csv  or _ask_path("è¼¸å…¥æª”ï¼ˆprocessed_logs.csv æˆ– sampled_logs.csvï¼‰", DEFAULT_INPUT)
    out_csv = out_csv or _ask_path("è¼¸å‡ºæª”ï¼ˆpreprocessed_data.csvï¼‰", DEFAULT_OUTPUT)
    uniq_p  = uniq_json or _ask_path("å”¯ä¸€å€¼æ¸…å–®ï¼ˆå¯æŒ‰ Enter ç•¥éï¼‰", DEFAULT_UNIQUE)

    # è‹¥ run_dir å­˜åœ¨ï¼Œå°‡ out_csv è½åœ°åˆ° 01_map
    if run_dir:
        dir_map = os.path.join(run_dir, "01_map")
        os.makedirs(dir_map, exist_ok=True)
        out_csv = out_csv if os.path.isabs(out_csv) else os.path.join(dir_map, os.path.basename(out_csv))

    if not os.path.exists(in_csv):
        if not QUIET:
            print(Fore.RED + f"âŒ æ‰¾ä¸åˆ°è¼¸å…¥æª”ï¼š{in_csv}")
        return

    uniq_map, do_check = _load_unique_values(uniq_p)
    first = True
    total = 0
    missing = {}

    import pandas as _pandas
    for chunk_pd in tqdm(_pandas.read_csv(in_csv, chunksize=CSV_CHUNK_SIZE, encoding="utf-8"),
                         desc=("åˆ†å¡Šè™•ç†" if not QUIET else None), unit="chunk", disable=QUIET):
        chunk = xdf.DataFrame.from_pandas(chunk_pd) if _HAS_CUDF else chunk_pd

        if "raw_log" in chunk.columns:
            try:
                chunk.drop(columns=["raw_log"], inplace=True)
            except Exception:
                pass

        if "datetime" in chunk.columns:
            try:
                chunk["datetime"] = xdf.to_datetime(chunk["datetime"], errors="coerce")
            except Exception:
                chunk["datetime"] = xdf.to_datetime(chunk["datetime"])

        if do_check:
            _check_coverage(chunk_pd, uniq_map, missing)

        chunk = _apply_mappings(chunk, uniq_map)
        if "is_attack" not in chunk.columns:
            # ä¿åº•ï¼šè‹¥ç¼ºå¤±å‰‡è£œ 0
            try:
                chunk["is_attack"] = 0
            except Exception:
                pass

        chunk = _reorder_preserve(chunk)

        # å¯«æª”
        mode = "w" if first else "a"
        chunk.to_csv(out_csv, mode=mode, header=first, index=False, encoding="utf-8")
        first = False
        total += len(chunk)

        check_and_flush("gpu_log_mapping", chunk)

    if not QUIET:
        print(Fore.GREEN + f"âœ… è¼¸å‡ºï¼š{out_csv}ï¼ˆ{total} ç­†ï¼‰")

    # å›å¯« manifest.map
    if run_dir:
        m = _read_manifest(run_dir)
        m.setdefault("map", {})
        m["map"]["input"] = os.path.abspath(in_csv)
        m["map"]["output"] = os.path.abspath(out_csv)
        try:
            with open(os.path.join(run_dir, "manifest.json"), "w", encoding="utf-8") as f:
                json.dump(m, f, ensure_ascii=False, indent=2)
            if not QUIET:
                print(Style.BRIGHT + Fore.GREEN + "ğŸ§­ manifest.map å·²æ›´æ–°")
        except Exception as e:
            if not QUIET:
                print(Fore.YELLOW + f"âš ï¸ manifest.map å¯«å…¥å¤±æ•—ï¼š{e}")
