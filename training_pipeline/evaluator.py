# training_pipeline/evaluator.py
from __future__ import annotations

from typing import Any, Dict, Optional
import numpy as np
import warnings

from sklearn.metrics import (
    accuracy_score,
    f1_score,
    precision_score,
    recall_score,
    roc_auc_score,
    classification_report,
    confusion_matrix,
)
try:
    from sklearn.exceptions import UndefinedMetricWarning
except Exception:
    UndefinedMetricWarning = Warning  # å…¼å®¹èˆŠç‰ˆ

class Evaluator:
    """
    ä¿ç•™åŸè¼¸å‡ºé¢¨æ ¼ï¼š
    âœ… æ¸¬è©¦é›†æº–ç¢ºç‡ï¼š
    ğŸ“ˆ OVR-AUCï¼š
    ğŸ“‹ åˆ†é¡å ±å‘Šï¼š
    ğŸ“ æ··æ·†çŸ©é™£ï¼š
    æ–°å¢ï¼šçµ±ä¸€ zero_division=0 ä¸¦éœéŸ³ UndefinedMetricWarningï¼ˆä¾‹å¦‚å°‘æ•¸é¡åˆ¥å®Œå…¨ç„¡é æ¸¬æ™‚ï¼‰
    """

    def __init__(self, task: str = "binary") -> None:
        self.task = task

    def evaluate(self, model: Any, X, y_true, name: Optional[str] = None) -> Dict[str, Any]:
        if name:
            print(f"\n==================== [ {name} è©•ä¼°çµæœ ] ====================")

        X_eval = X.to_numpy() if hasattr(X, "to_numpy") else np.asarray(X)
        y_true_np = y_true.to_numpy().reshape(-1) if hasattr(y_true, "to_numpy") else np.asarray(y_true).reshape(-1)

        # é æ¸¬
        y_pred = model.predict(X_eval)

        # ====== åˆ†é¡å ±å‘Šèˆ‡åŸºæœ¬æŒ‡æ¨™ï¼ˆéœéŸ³ UndefinedMetricWarningï¼‰ ======
        with warnings.catch_warnings():
            warnings.filterwarnings("ignore", category=UndefinedMetricWarning)
            acc = accuracy_score(y_true_np, y_pred)
            if self.task == "binary":
                f1 = f1_score(y_true_np, y_pred, average="binary", zero_division=0)
                prec = precision_score(y_true_np, y_pred, average="binary", zero_division=0)
                rec = recall_score(y_true_np, y_pred, average="binary", zero_division=0)
                report = classification_report(y_true_np, y_pred, digits=4, zero_division=0)
            else:
                f1 = f1_score(y_true_np, y_pred, average="macro", zero_division=0)
                prec = precision_score(y_true_np, y_pred, average="macro", zero_division=0)
                rec = recall_score(y_true_np, y_pred, average="macro", zero_division=0)
                report = classification_report(y_true_np, y_pred, digits=4, zero_division=0)

        # ====== AUCï¼ˆbinary / multiclass OVRï¼‰ ======
        auc = np.nan
        try:
            if hasattr(model, "predict_proba"):
                proba = model.predict_proba(X_eval)
                if self.task == "binary":
                    # å–æ­£é¡ç´¢å¼•ç‚º 1ï¼›è‹¥ classes_ ä¸å« 1ï¼Œç”¨æœ€å¾Œä¸€é¡
                    pos_idx = 1
                    if hasattr(model, "classes_"):
                        cls = list(model.classes_)
                        pos_idx = cls.index(1) if 1 in cls else (len(cls) - 1)
                    auc = roc_auc_score(y_true_np, proba[:, pos_idx])
                else:
                    auc = roc_auc_score(y_true_np, proba, multi_class="ovr")
        except Exception:
            pass  # ä¿ç•™ auc=nan

        # ====== è¼¸å‡º ======
        print(f"âœ… æ¸¬è©¦é›†æº–ç¢ºç‡ï¼š {acc:.6f}")
        if self.task == "binary":
            print(f"ğŸ“ˆ AUCï¼š {auc if np.isnan(auc) else f'{auc:.6f}'}")
        else:
            print(f"ğŸ“ˆ OVR-AUCï¼š {auc if np.isnan(auc) else f'{auc:.6f}'}")
        print("ğŸ“‹ åˆ†é¡å ±å‘Šï¼š")
        print(report)

        cm = confusion_matrix(y_true_np, y_pred)
        print("ğŸ“ æ··æ·†çŸ©é™£ï¼š")
        print(cm)

        # é¡å¤–ï¼šåˆ—å‡ºæ¯ä¸€é¡è¢«é æ¸¬çš„æ•¸é‡ï¼ˆå¹«åŠ©è§€å¯Ÿã€Œå®Œå…¨æ²’é æ¸¬åˆ°æŸé¡ã€çš„ç‹€æ³ï¼‰
        try:
            unique, counts = np.unique(y_pred, return_counts=True)
            dist = dict(zip(unique.tolist(), counts.tolist()))
            print(f"ğŸ“¦ é æ¸¬é¡åˆ¥åˆ†ä½ˆï¼š{dist}")
        except Exception:
            pass

        return {
            "acc": acc,
            "auc": auc,
            "f1": f1,
            "precision": prec,
            "recall": rec,
            "report": report,
            "confusion_matrix": cm,
        }
