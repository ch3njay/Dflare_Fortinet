# -*- coding: utf-8 -*-
"""
pipeline_controller.py
ç›®çš„ï¼š
- ä¸²æ¥ log_cleaning / log_mapping / feature_engineering ä¸‰éšæ®µ
- åŒæª”åŒä»‹é¢æ”¯æ´ CLI èˆ‡ UIï¼ˆç¨‹å¼åŒ–ï¼‰å…©ç¨®ç”¨æ³•
- å¤§æª”æµå¼è™•ç†ã€é€²åº¦æ¢ã€è‰²å½©ã€é˜²ç¬¨

ç›¸ä¾ï¼š
- log_cleaning.py: clean_logs()ï¼ˆäº’å‹•å¼ï¼‰
- log_mapping.py: _apply_mappings(), _reorder_preserve(), _load_unique_values(), _check_coverage(), å¸¸æ•¸
- feature_engineering.py: å„ add_* å‡½å¼èˆ‡å¸¸æ•¸è¨­å®š

ä½¿ç”¨ï¼š
CLIï¼š python pipeline_controller.py
UI / ç¨‹å¼åŒ–ï¼šfrom pipeline_controller import run_pipeline
"""

import os
import json
import pandas as pd
from typing import Optional, Dict, Any
from tqdm import tqdm
from colorama import Fore, Style, init as colorama_init

# åˆå§‹åŒ–è‰²å½©
colorama_init(autoreset=True)

# åŒ¯å…¥å€‹æ—¢æœ‰æ¨¡çµ„
from etl_pipeline import log_cleaning as LC   # æä¾› clean_logs() äº’å‹•æµç¨‹
from etl_pipeline import log_mapping as LM    # æä¾›æ˜ å°„èˆ‡æ’åºçš„å·¥å…·æ–¹æ³•
from etl_pipeline import feature_engineering as FE  # æä¾›äº”å¤§é¡ç‰¹å¾µå·¥ç¨‹æ–¹æ³•
from etl_pipeline.utils import check_and_flush

# å…¨åŸŸéœé»˜æ¨¡å¼ï¼ˆéäº’å‹•å‘¼å«æ™‚å¯é¿å…å¤šé¤˜æç¤ºï¼‰
LC.QUIET = False
LM.QUIET = False
FE.QUIET = False
# ------------------------- å…¨åŸŸè¨­å®š -------------------------
# è¨˜æ†¶é«”ä½¿ç”¨ç‡é–¾å€¼ï¼ˆè¶…éæ­¤å€¼æ‰è§¸ç™¼ flushï¼‰
MEMORY_FLUSH_THRESHOLD = 80.0  # è®Šæ›´æ­¤å€¼å¯èª¿æ•´å…¨æµç¨‹ flush é–¾å€¼
# ------------------------- é è¨­åƒæ•¸ -------------------------
# é è¨­è·¯å¾‘
DEFAULT_CLEAN_OUT   = "processed_logs.csv"
DEFAULT_PREPROC_OUT = "preprocessed_data.csv"
DEFAULT_FE_OUT      = "engineered_data.csv"
DEFAULT_UNIQUE_JSON = "log_unique_values.json"

# I/O è¨­å®š
CSV_CHUNK_SIZE = 100_000
CSV_ENCODING   = "utf-8"

# ------------------------- å·¥å…· -------------------------
def _ask_yn(prompt: str, default: bool) -> bool:
    while True:
        s = input(Fore.CYAN + f"{prompt} (1=æ˜¯,0=å¦ï¼›é è¨­ {1 if default else 0})ï¼š").strip()
        if s == "": return default
        if s in ("0","1"): return s == "1"
        print(Fore.RED + "âŒ è¼¸å…¥éŒ¯èª¤ï¼Œè«‹é‡æ–°è¼¸å…¥ï¼")

def _ask_path(prompt: str, default_path: str) -> str:
    p = input(Fore.CYAN + f"{prompt}ï¼ˆé è¨­ {default_path}ï¼‰ï¼š").strip()
    return p if p else default_path

def _ensure_datetime(col):
    # å°‡ DataFrame çš„ datetime æ¬„ä½è½‰ç‚ºçœŸæ­£çš„ datetime å‹åˆ¥ï¼ˆè‹¥å­˜åœ¨ï¼‰
    if "datetime" in col.columns and not pd.api.types.is_datetime64_any_dtype(col["datetime"]):
        col["datetime"] = pd.to_datetime(col["datetime"], errors="coerce")
    return col

def _resolve_out_path(in_csv: str, out_csv: str) -> str:
    """è‹¥ out_csv æœªå«è³‡æ–™å¤¾ï¼Œæ”¹å¯«åˆ° in_csv åŒè³‡æ–™å¤¾ã€‚"""
    out_csv = os.path.expanduser(out_csv)
    if os.path.isabs(out_csv):
        return out_csv
    base_dir = os.path.dirname(os.path.abspath(in_csv)) if in_csv else os.getcwd()
    return os.path.join(base_dir, out_csv)

# ------------------------- S2ï¼šæ˜ å°„ï¼ˆéäº’å‹•ï¼Œä¾› UI ç”¨ï¼‰ -------------------------
def run_mapping_noninteractive(
    in_csv: str,
    out_csv: str = DEFAULT_PREPROC_OUT,
    unique_json: Optional[str] = DEFAULT_UNIQUE_JSON
) -> str:
    """
    éäº’å‹•ç‰ˆæœ¬çš„æ˜ å°„èˆ‡æ’åºï¼ˆç›´æ¥é‡ç”¨ log_mapping å…§éƒ¨æ–¹æ³•ï¼‰ã€‚
    - åƒ…åšå­—å…¸æ˜ å°„èˆ‡æ¬„ä½æ’åº
    - æª¢æŸ¥å”¯ä¸€å€¼è¦†è“‹ï¼ˆè‹¥æä¾› unique_jsonï¼‰
    """
    if not os.path.exists(in_csv):
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°è¼¸å…¥æª”ï¼š{in_csv}")
    
    out_csv = _resolve_out_path(in_csv, out_csv)
    
    uniq_map, do_check = ({}, False)
    if unique_json and os.path.exists(unique_json):
        uniq_map, do_check = LM._load_unique_values(unique_json)  # ä½¿ç”¨ç¾æœ‰æ–¹æ³•

    first = True
    total = 0
    missing = {}

    for chunk in tqdm(pd.read_csv(in_csv, chunksize=CSV_CHUNK_SIZE, encoding=CSV_ENCODING),
                    desc="æ˜ å°„åˆ†å¡Š", unit="chunk"):
        if "raw_log" in chunk.columns:
            chunk.drop(columns=["raw_log"], inplace=True)
        chunk = _ensure_datetime(chunk)

        # è¦†è“‹æª¢æŸ¥è¦åœ¨æ˜ å°„å‰ï¼ˆservice é‚„æ˜¯å­—ä¸²ï¼‰
        if do_check:
            LM._check_coverage(chunk, uniq_map, missing)

        # é€™è£¡æŠŠ uniq_map å‚³é€²å»ï¼Œç¢ºä¿ service ç©©å®šæ˜ å°„
        chunk = LM._apply_mappings(chunk, uniq_map)

        if "is_attack" not in chunk.columns:
            if "crscore" in chunk.columns:
                chunk["is_attack"] = (pd.to_numeric(chunk["crscore"], errors="coerce")
                                    .fillna(0).astype(int) > 0).astype(int)
            else:
                chunk["is_attack"] = 0

        chunk.drop_duplicates(inplace=True)
        chunk = LM._reorder_preserve(chunk)

        chunk.to_csv(out_csv, mode="w" if first else "a", header=first,
                    index=False, encoding=CSV_ENCODING)
        first = False
        total += len(chunk)

    # å ±å‘Š
    report_path = os.path.splitext(out_csv)[0] + "_mapping_report.json"
    rep = {"total_rows": total}
    if missing:
        rep["uncovered_values"] = {k: sorted(list(v)) for k, v in missing.items()}
    else:
        rep["uncovered_values"] = "none or not-checked"
    with open(report_path, "w", encoding="utf-8") as f:
        json.dump(rep, f, ensure_ascii=False, indent=2)

    print(Fore.GREEN + f"âœ… æ˜ å°„å®Œæˆï¼š{out_csv}ï¼ˆ{total} ç­†ï¼‰")
    print(Fore.GREEN + f"ğŸ“ å ±å‘Šï¼š{report_path}")
    return out_csv

# ------------------------- S3ï¼šç‰¹å¾µå·¥ç¨‹ï¼ˆéäº’å‹•ï¼Œä¾› UI ç”¨ï¼‰ -------------------------
def run_feature_engineering_noninteractive(
    in_csv: str,
    out_csv: str = DEFAULT_FE_OUT,
    # ä¾ç…§ FE æ¨¡çµ„çš„é–‹é—œè¨­è¨ˆï¼Œæä¾›å¯è¦†å¯«åƒæ•¸
    enable_traffic_stats: Optional[bool] = None,
    enable_proto_port: Optional[bool] = None,
    enable_windowed: Optional[bool] = None,
    enable_rel_base: Optional[bool] = None,
    enable_rel_topk: Optional[bool] = None,
    enable_anomaly: Optional[bool] = None,
    topk_src_port_json: Optional[str] = None,
    topk_pair_json: Optional[str] = None,
) -> str:
    """
    éäº’å‹•ç‰ˆæœ¬çš„ç‰¹å¾µå·¥ç¨‹ï¼ˆé‡ç”¨ feature_engineering å…§éƒ¨æ–¹æ³•èˆ‡å¸¸æ•¸ï¼‰ã€‚
    å¯ç”¨åƒæ•¸è¦†å¯« FE çš„é è¨­é–‹é—œèˆ‡ top-k å­—å…¸è·¯å¾‘ã€‚
    """
    if not os.path.exists(in_csv):
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°è¼¸å…¥æª”ï¼š{in_csv}")
    out_csv = _resolve_out_path(in_csv, out_csv)
    
    # ä»¥åƒæ•¸è¦†å¯« FE æ¨¡çµ„å…§çš„æ——æ¨™ï¼ˆè‹¥æœ‰æä¾›ï¼‰
    if enable_traffic_stats is not None:  FE.ENABLE_TRAFFIC_STATS    = enable_traffic_stats
    if enable_proto_port is not None:     FE.ENABLE_PROTO_PORT_FEATS = enable_proto_port
    if enable_windowed is not None:       FE.ENABLE_WINDOWED_FEATS   = enable_windowed
    if enable_rel_base is not None:       FE.ENABLE_RELATIONAL_BASE  = enable_rel_base
    if enable_rel_topk is not None:       FE.ENABLE_RELATIONAL_TOPK  = enable_rel_topk
    if enable_anomaly is not None:        FE.ENABLE_ANOMALY_INDIC    = enable_anomaly

    if topk_src_port_json: FE.TOPK_SRC_PORT_JSON = topk_src_port_json
    if topk_pair_json:     FE.TOPK_PAIR_JSON     = topk_pair_json

    # è¼‰å…¥ Top-K å­—å…¸ï¼ˆè‹¥æ²’æœ‰å°±å›å‚³ Noneï¼ŒFE å…§éƒ¨æœƒå®‰å…¨è·³éï¼‰
    topk_src_port = FE._load_json_if_exists(FE.TOPK_SRC_PORT_JSON)
    topk_pair     = FE._load_json_if_exists(FE.TOPK_PAIR_JSON)

    first = True
    total = 0
    state: Dict[str, Any] = {}  # çµ¦æ™‚é–“çª—ç‰¹å¾µè·¨ chunk çš„å°ç‹€æ…‹

    for chunk in tqdm(pd.read_csv(in_csv, chunksize=CSV_CHUNK_SIZE, encoding=CSV_ENCODING),
                      desc="å·¥ç¨‹åˆ†å¡Š", unit="chunk"):
        # æ™‚é–“æ¬„ä½å‹åˆ¥ä¿éšª
        chunk = _ensure_datetime(chunk)

        # 1) æµé‡çµ±è¨ˆ
        if FE.ENABLE_TRAFFIC_STATS:
            chunk = FE.add_traffic_stats(chunk)

        # 2) å”å®š/ç«¯å£
        if FE.ENABLE_PROTO_PORT_FEATS:
            chunk = FE.add_proto_port_feats(chunk)

        # 3) æ™‚é–“çª—å£ï¼ˆå¯é¸ï¼‰
        if FE.ENABLE_WINDOWED_FEATS:
            chunk = FE.add_windowed_feats(chunk, state)

        # 4) é—œä¿‚ç‰¹å¾µ
        if FE.ENABLE_RELATIONAL_BASE:
            chunk = FE.add_relational_basic(chunk)
        if FE.ENABLE_RELATIONAL_TOPK:
            chunk = FE.add_relational_topk(chunk, topk_src_port, topk_pair)

        # 5) ç•°å¸¸æŒ‡æ¨™
        if FE.ENABLE_ANOMALY_INDIC:
            chunk = FE.add_anomaly_indicators(chunk)

        # 6) å·¥ç¨‹å¾Œé¡åˆ¥æ¬„ä½æ•¸å€¼åŒ–ï¼ˆè‹¥æœ‰ï¼‰
        if getattr(FE, "ENCODE_ENGINEERED_CATS", False) and hasattr(FE, "encode_engineered_categoricals"):
            chunk = FE.encode_engineered_categoricals(chunk)
        
        # æ ¸å¿ƒåœ¨å‰ï¼Œæ–°ç‰¹å¾µé™„åœ¨å¾Œï¼›å»é‡
        chunk = FE._reorder_append(chunk)
        chunk.drop_duplicates(inplace=True)

        # å¯«å‡º
        chunk.to_csv(out_csv, mode="w" if first else "a", header=first,
                     index=False, encoding=CSV_ENCODING)
        first = False
        total += len(chunk)

    print(Fore.GREEN + f"âœ… ç‰¹å¾µå·¥ç¨‹å®Œæˆï¼š{out_csv}ï¼ˆ{total} ç­†ï¼‰")
    return out_csv

# ------------------------- ç¸½ç®¡ï¼šCLI èˆ‡ UI çš†å¯ -------------------------
def run_pipeline(
    do_clean: bool = True,
    do_map: bool = True,
    do_fe: bool = False,
    # I/O è·¯å¾‘ï¼ˆå¯ç‚º None â†’ ä¾å‰ä¸€æ­¥è¼¸å‡ºæ±ºå®šï¼‰
    clean_out: str = DEFAULT_CLEAN_OUT,
    preproc_out: str = DEFAULT_PREPROC_OUT,
    fe_out: str = DEFAULT_FE_OUT,
    unique_json: Optional[str] = DEFAULT_UNIQUE_JSON,
    # FE åƒæ•¸ï¼ˆä¾› UI/ç¨‹å¼åŒ–è¦†å¯«ï¼‰
    fe_enable: Optional[Dict[str, bool]] = None,
    fe_topk_src_port_json: Optional[str] = None,
    fe_topk_pair_json: Optional[str] = None
) -> str:
    """
    UI/ç¨‹å¼åŒ–å…¥å£ï¼šä»¥åƒæ•¸æ±ºå®šå„éšæ®µæ˜¯å¦åŸ·è¡Œèˆ‡è¼¸å…¥è¼¸å‡ºè·¯å¾‘ã€‚
    - æ¸…æ´—éšæ®µï¼šå‘¼å« LC.clean_logs()ï¼ˆäº’å‹•ï¼‰ï¼›æˆ–ä½¿ç”¨è€…å¯å…ˆè¡Œç”¢å‡º processed_logs.csv å†åªè·‘å¾Œå…©éšæ®µ
    - æ˜ å°„èˆ‡ç‰¹å¾µå·¥ç¨‹ï¼šçš†ä½¿ç”¨éäº’å‹•ç‰ˆæœ¬ï¼ˆæœ¬æª”æä¾›ï¼‰ï¼Œä¸æœƒå½ˆçª—
    å›å‚³ï¼šæœ€çµ‚è¼¸å‡ºæª”è·¯å¾‘
    """
    current_path = None

    # S1 æ¸…æ´—ï¼ˆè‹¥å•Ÿç”¨ï¼Œèµ°åŸæ¨¡çµ„äº’å‹•æµç¨‹ï¼›è¼¸å‡ºæª”åå¯åœ¨äº’å‹•ä¸­æŒ‡å®šï¼‰
    if do_clean:
        print(Style.BRIGHT + "â€”â€” ç¬¬ 1 éšæ®µï¼šæ¸…æ´— / æ¨™æº–åŒ– â€”â€”")
        current_path = LC.clean_logs()  # äº’å‹•å¼ï¼›æœƒå›å‚³å¯¦éš›è¼¸å‡ºè·¯å¾‘  :contentReference[oaicite:6]{index=6}
        check_and_flush("pipeline_controller_after_cleaning")
    else:
        # è‹¥æœªåŸ·è¡Œæ¸…æ´—ï¼Œé è¨­ç”¨æŒ‡å®šä¹‹ processed_logs.csv
        current_path = clean_out
        if not os.path.exists(current_path):
            raise FileNotFoundError(f"æ‰¾ä¸åˆ° processed_logsï¼š{current_path}")

    # S2 æ˜ å°„ï¼ˆéäº’å‹•ï¼›å®‰å…¨ä¸å½ˆçª—ï¼‰
    if do_map:
        print(Style.BRIGHT + "â€”â€” ç¬¬ 2 éšæ®µï¼šå­—å…¸æ˜ å°„ / æ¬„ä½æ’åº â€”â€”")
        current_path = run_mapping_noninteractive(
            in_csv=current_path,
            out_csv=preproc_out,
            unique_json=unique_json
        )
        check_and_flush("pipeline_controller_after_mapping")
    else:
        current_path = preproc_out
        if not os.path.exists(current_path):
            raise FileNotFoundError(f"æ‰¾ä¸åˆ° preprocessed_dataï¼š{current_path}")

    # S3 ç‰¹å¾µå·¥ç¨‹ï¼ˆéäº’å‹•ï¼‰
    if do_fe:
        print(Style.BRIGHT + "â€”â€” ç¬¬ 3 éšæ®µï¼šç‰¹å¾µå·¥ç¨‹ â€”â€”")
        fe_kwargs = {}
        if fe_enable:
            fe_kwargs.update(dict(
                enable_traffic_stats=fe_enable.get("traffic_stats"),
                enable_proto_port=fe_enable.get("proto_port"),
                enable_windowed=fe_enable.get("windowed"),
                enable_rel_base=fe_enable.get("rel_base"),
                enable_rel_topk=fe_enable.get("rel_topk"),
                enable_anomaly=fe_enable.get("anomaly"),
            ))
        current_path = run_feature_engineering_noninteractive(
            in_csv=current_path,
            out_csv=fe_out,
            topk_src_port_json=fe_topk_src_port_json,
            topk_pair_json=fe_topk_pair_json,
            **fe_kwargs
        )
        check_and_flush("pipeline_controller_after_feature_eng") 

    print(Fore.GREEN + f"âœ… Pipeline å®Œæˆã€‚æœ€çµ‚è¼¸å‡ºï¼š{current_path}")
    return current_path

# ------------------------- CLI å…¥å£ -------------------------
def run_pipeline_cli():
    print(Style.BRIGHT + "==== D-FLARE Pipeline æ§åˆ¶å™¨ï¼ˆCLIï¼‰====")
    do_clean = _ask_yn("æ˜¯å¦åŸ·è¡Œ ç¬¬1éšæ®µï¼šæ¸…æ´—/æ¨™æº–åŒ–ï¼ˆlog_cleaningï¼‰", True)
    do_map   = _ask_yn("æ˜¯å¦åŸ·è¡Œ ç¬¬2éšæ®µï¼šå­—å…¸æ˜ å°„/æ’åºï¼ˆlog_mappingï¼‰", True)
    do_fe    = _ask_yn("æ˜¯å¦åŸ·è¡Œ ç¬¬3éšæ®µï¼šç‰¹å¾µå·¥ç¨‹ï¼ˆfeature_engineeringï¼‰", False)

    # è·¯å¾‘è¨­å®šï¼ˆç•¶æœªåŸ·è¡Œå‰ä¸€æ­¥æ™‚éœ€äººå·¥æŒ‡å®šï¼‰
    clean_out   = _ask_path("ç¬¬1éšæ®µè¼¸å‡ºï¼ˆprocessed_logs.csvï¼‰", DEFAULT_CLEAN_OUT) if not do_clean else DEFAULT_CLEAN_OUT
    preproc_out = _ask_path("ç¬¬2éšæ®µè¼¸å‡ºï¼ˆpreprocessed_data.csvï¼‰", DEFAULT_PREPROC_OUT) if not do_map else DEFAULT_PREPROC_OUT
    fe_out      = _ask_path("ç¬¬3éšæ®µè¼¸å‡ºï¼ˆengineered_data.csvï¼‰", DEFAULT_FE_OUT)

    unique_json = _ask_path("å”¯ä¸€å€¼æ¸…å–®ï¼ˆæŒ‰ Enter è·³éï¼‰", DEFAULT_UNIQUE_JSON)

    # FE é¸é …
    fe_enable = None
    fe_topk_src = None
    fe_topk_pair = None
    if do_fe:
        print(Style.BRIGHT + "â€”â€” ç‰¹å¾µå·¥ç¨‹é–‹é—œï¼ˆè¼¸å…¥ 1=é–‹,0=é—œï¼ŒEnter=é è¨­ï¼‰â€”â€”")
        def ask_flag(q, default):
            s = input(Fore.CYAN + f"{q}ï¼ˆé è¨­ {1 if default else 0}ï¼‰ï¼š").strip()
            if s == "": return None  # ä½¿ç”¨é è¨­
            return (s == "1")
        fe_enable = {
            "traffic_stats": ask_flag("1) æµé‡çµ±è¨ˆ", FE.ENABLE_TRAFFIC_STATS),
            "proto_port":    ask_flag("2) å”å®š/ç«¯å£", FE.ENABLE_PROTO_PORT_FEATS),
            "windowed":      ask_flag("3) æ™‚é–“çª—å£", FE.ENABLE_WINDOWED_FEATS),
            "rel_base":      ask_flag("4a) é—œä¿‚ç‰¹å¾µï¼ˆåŸºç¤ï¼‰", FE.ENABLE_RELATIONAL_BASE),
            "rel_topk":      ask_flag("4b) é—œä¿‚ç‰¹å¾µï¼ˆTop-Kï¼‰", FE.ENABLE_RELATIONAL_TOPK),
            "anomaly":       ask_flag("5) ç•°å¸¸æŒ‡æ¨™", FE.ENABLE_ANOMALY_INDIC),
        }
        fe_topk_src = _ask_path("Top-K å­—å…¸ï¼ˆsrcipâ†’dstport JSONï¼ŒEnter è·³éï¼‰", FE.TOPK_SRC_PORT_JSON)
        fe_topk_pair= _ask_path("Top-K å­—å…¸ï¼ˆsrcipâ†’dstip JSONï¼ŒEnter è·³éï¼‰", FE.TOPK_PAIR_JSON)

    # åŸ·è¡Œ
    return run_pipeline(
        do_clean=do_clean, do_map=do_map, do_fe=do_fe,
        clean_out=clean_out, preproc_out=preproc_out, fe_out=fe_out,
        unique_json=unique_json if os.path.exists(unique_json) else None,
        fe_enable=fe_enable,
        fe_topk_src_port_json=fe_topk_src if fe_topk_src and os.path.exists(fe_topk_src) else None,
        fe_topk_pair_json=fe_topk_pair if fe_topk_pair and os.path.exists(fe_topk_pair) else None
    )

if __name__ == "__main__":
    run_pipeline_cli()
